	RQ1. Symptoms				RQ2. Causes		RQ3. Repairs		RQ4. Impacts
id	description of symptom	symptom type	related issue	related issue status	description of cause	cause  type	description of repair	repair type	url of doc
ZOOKEEPER-3151	Jenkins github integration is broken if retriggering the precommit job through Jenkins admin web page.	3. Build and testing error	none	none		2.7 Problems in settings	Provided work around in?https://cwiki.apache.org/confluence/display/ZOOKEEPER/HowToContribute	2.1 Modifying documents	https://cwiki.apache.org/confluence/display/ZOOKEEPER/HowToContribute
ZEPPELIN-4002	Zeppelin is unable to resolve full interpreter name at the beginning of paragraph. It doesn't resolve anything after hyphen.	2. Unexpected behavior	ZEPPELIN-4008	open	We enforce interpreter name could only contain alphabet and under_score like you define variables in most of programming languages in zeppelin 0.8	3.3 Incorrect inputs	We enforce interpreter name could only contain alphabet and under_score like you define variables in most of programming languages in zeppelin 0.8. One workaround for you is to rename your interpreter.	3.10 Modifying input values	https://zeppelin.apache.org/docs/0.8.0/usage/interpreter/overview.html
ZEPPELIN-3918	[ERROR] Failed to execute goal on project zeppelin-beam	1. Crash	none	none	"It seems that some resources are not available

The beam library is too old the release for 2.11 was removed from maven repo."	3.1 Problems in my libraries	"Found a Solution: Disabling the beam Module

$> mvn clean package -U -DskipTests -Dhadoop3 -Pspark-2.4 -Pscala-2.11 -pl '!beam' | tee build.log"	3.3 Switching to newer versions	
ZEPPELIN-3195	"Need to set ""ZEPPELIN_INTERPRETER_MAX_POOL_SIZE"" with large number."	2. Unexpected behavior	none	none	"At the current moment the parameter ""ZEPPELIN_INTERPRETER_MAX_POOL_SIZE"" limits the number of paragraphs that executed at cron.
Need to set ""ZEPPELIN_INTERPRETER_MAX_POOL_SIZE"" with large number."	2.7 Problems in settings	"I renewed docs: remove old ""ZEPPELIN_INTERPRETERS"" and added ZEPPELIN_INTERPRETER_MAX_POOL_SIZE.
The default value for ZEPPELIN_INTERPRETER_MAX_POOL_SIZE remains 10."	2.1 Modifying documents	
ZEPPELIN-3175	Thus when analysts run 100 jvm/interpreter processes then Zeppelin will be fully stuck	"4. Hang

"	none	none	"Non SchedulerFactory allow to create only 100 schedulers. Schedulers aren't shared, there is one scheduler per each interpreter processes."	2.7 Problems in settings	"I had to test the bug before filing the PR.
I saw this bug in branch-0.7 and master branch several month ago.
It's excellent this bug is absent now.

pr is not merged"	1.1 Fixed problems	
ZEPPELIN-2877	Apache Zeppelin 0.8 SNAPSHOT build from source failed on Ubuntu	3. Build and testing error	none	none	"Looks like Scala version 2.11 is the reason why the build is failing. To fix the problem, I have used following configuration.

./dev/change_scala_version.sh 2.10"	3.1 Problems in my libraries	"To fix the problem, I have used following configuration.

./dev/change_scala_version.sh 2.10

# build zeppelin
mvn clean package -Pbuild-distr -Pspark-2.2 -DskipTests"	2.3 Modifying build files or options	
ZEPPELIN-1966	"Lets suppose user1 has created Notebook 9 and he has not changed the note permissions.
And the user2 logins, since the Notebook 9 is visible to user2, he can change the
permissions of the user1 note(Notebook 9)."	5. Security threat	none	none	your question has been solved by this PR£ºhttps://github.com/apache/zeppelin/pull/1625	1.1 Fixed by related issues	your question has been solved by this PR£ºhttps://github.com/apache/zeppelin/pull/1625£¬ and zeppelin 0.7 solved the problem	3.3 Switching to newer versions	
YARN-8279	logaggregation.AggregatedLogDeletionService (AggregatedLogDeletionService.java:logIOException(182)) - Could not read the contents of hdfs://prabhucluster:8020/app-logs/hive/logs	1. Crash	none	none	AggregationLogDeletionService does not honor yarn.log-aggregation.IndexedFormat.remote-app-log-dir-suffix. AggregationLogService writes the logs into /app-logs/<user-dir>/logs-ifile where as AggregationLogDeletion tries to delete from /app-logs/<user-dir>/logs.	2.7 Problems in settings	"The fix is to set the below properties in yarn-site.xml and then restart Mapreduce HistoryServer and YARN services. 
yarn.log-aggregation.IndexedFormat.remote-app-log-dir-suffix and yarn.nodemanager.remote-app-log-dir-suffix to same value ""logs-ifile"" which serves AggregationLogDeletionServic"	2.2 Modifying my settings	
YARN-5381	NPE can be thrown when wildcard is used in libjar option and if the cluster is secure.	1. Crash	none	none	"Per Daniel Templeton's comment in YARN-5373, a windows implementation is no longer needed as PrivilegedOperation is used. Closing this."	1.1 Fixed by related issues	"Per Daniel Templeton's comment in YARN-5373, a windows implementation is no longer needed as PrivilegedOperation is used. Closing this.

I have a fix working that uses a PrivilegedOperation instead of modifying the script. Haibo Chen, you can close the Windows JIRA. "	1.1 Fixed problems	
XMLBEANS-512	"In SaxHandler, if an XML entity exceeds the entity byte limit, an exception is thrown. Too bad the counter is not being reset, so when the same SaxHandler parses another, valid XML message, the same exception is thrown again."	1. Crash	none	none	"This only happens when using the default XMLReader Piccolo

piccolo support was removed in v3.0.0

Release: Apache XMLBeans 2.0.0 released! (30 June 2005)
XML parsing is now by default performed by Piccolo, a high performance parser

Release: Apache XMLBeans 3.0.0 (June 29, 2018)
XMLBEANS-515: remove piccolo support"	3.1 Problems in my libraries	We no longer support Piccolo - upgrade to XMLBeans 3.+	3.3 Switching to newer versions	http://xmlbeans.apache.org/news.html
WEEX-229	the value attribute is not working 	2. Unexpected behavior	none	none	 text components value attribute not effective	3.3 Incorrect inputs	"The conclusion is that it's not likely to support 'value' attribute for 'text' component yet in weex-vue-render@1.x. You should put your text value as a child of text node. e.g. Use '<text>test</text>' instead of '<text value=""test""></text>'."	3.11 Modifying input format	https://weex.apache.org/docs/components/text.html#attributes
TOREE-428	org.apache.spark.SparkDriverExecutionException: Execution error   Caused by: java.lang.ArrayStoreException	1. Crash	none	none	"I'm not sure why this works, but given the work-around is so easy, we are planning to wait for Scala 2.12 or just move away from the standard Scala interpreter.

this was a limitation on Scala 2.11 so closing this"	"1.2 Fixed by newer versions
4.2 Problems in languages"	"The work-around is to create the case class in a separate cell. I'm not sure why this works, but given the work-around is so easy, we are planning to wait for Scala 2.12 or just move away from the standard Scala interpreter."	2.3 Modifying build files or options	
TOREE-411	"I am trying to work with Hive tables but is impossible to see it with toree.
It works perfectly in pyspark kernel (ipykernel), but in toree kernel it returns an empty list."	2. Unexpected behavior	none	none	The root cause is found in Spark. Toree starts the default SparkSession object without Hive support.	3.1 Problems in my libraries	"The workaround is to start the three kernel in hive catalog mode.
Add this property into spark-defaults.conf"	2.2 Modifying my settings	
TOREE-353	I cannot see Hive tables inside Toree kernel.	2. Unexpected behavior	none	none	"Hive 2.1.0 may be causing some issues. Spark by default uses hive 1.2 for the metastore. 

Can you close this and ask your questions on the Spark mailing list as this is not part of Toree."	3.1 Problems in my libraries	Can you close this and ask your questions on the Spark mailing list as this is not part of Toree.	1.2 Unfixed problems	
TOMEE-2183	org.apache.commons.codec filtered by the classloader : java.lang.ClassNotFoundException: org.apache.commons.codec.binary.Base64	1. Crash	none	none	"While debugging I saw that ""org.apache.commons.codec"" is intentionally filtered by the classloader"	2.7 Problems in settings	"openejb.classloader.forced-load=org.apache.commons.codec # in conf/system.properties
should allow you to workaround it.

This is being worked-around using 
openejb.classloader.forced-load=org.apache.commons.codec"	2.2 Modifying my settings	https://tomee.apache.org/refcard/refcard.html
TOMEE-2181	when I try to load the PKCS12 file I'm getting a java.security.UnrecoverableKeyException: failed to decrypt safe contents entry: javax.crypto.BadPaddingException: pad block corrupted	1. Crash	none	none	" I add Firebird and MySQL datasources. If I only add their drivers to the lib folder it still works, but by the time I reference a Firebird or a MySQL datasource on tomee.xml I get the keystore error,

it only happens on tomee plus (not webprofile) with JMS activated cause of bouncycastle which is registered in position 2 by default. "	2.7 Problems in settings	You can tune it to be later in the chain (org.apache.activemq.broker.BouncyCastlePosition = 8 for instance in conf/system.properties) and the JVM security provider will be taken and work as expected.	2.2 Modifying my settings	
TOBAGO-572	"The first time the page is rendered correctly, but if you go back you will get an exception
java.lang.IllegalStateException: Client-id : _id54 is duplicated in the faces tree. Component : page:_id54, path:
{Component-Path : [Class: org.apache.myfaces.tobago.component.UIViewRoot,ViewId: /helloWorld.xhtml][Class: org.apache.myfaces.tobago.component.UIPage,Id: page][Class: org.apache.myfaces.tobago.component.UICell,Id: _id27][Class: org.apache.myfaces.tobago.component.UIBox,Id: _id29][Class: org.apache.myfaces.tobago.component.UICell,Id: _id54]}"	1. Crash	none	none	I think facelets has a bug if you are using component binding and embedded components.	4.2 Problems in languages	 I tried to define the binding attributes with <f:attribute> like this	3.11 Modifying input format	http://myfaces.apache.org/tobago/tobago-core/tlddoc/tc/selectOneChoice.html
TOBAGO-319	On click the popup opens but the action inside the popup box is not executed.	2. Unexpected behavior	none	none		3.2 Problems in my clients	Can you try it with popupReference to a popup outside the sheet.	3.9 Modifying API calls	https://myfaces.apache.org/tobago/doc/1.0.28/tld/tc/popupReference.html
TINKERPOP-2286	"Our provider version only supports string id, however when loading data with numeric ids via g.io().read().iterate() the DB didn't error out, but assigned random ids to those vertices with numeric ids, causing regression."	2. Unexpected behavior	none	none	"The Reporter thinks if the instance only allow string id and the id is not a string, it should error out  :
if the user import graph from a source which has numeric ids into a target only allows string id. The target graph would replace the numeric id with some internal ids and won't raise any exception to the user. The user would assume the graph import was successful and tries to get the vertices with the old ids but then realized that all the ids have been changed with some random internal ids. I think it would cause a great pain for the users."	2.2 Incompatible issues	"I tried the method to throw exception from willAllowId(), the result looks good. Please feel free to close this issue."	3.5 Overridden APIs	
TINKERPOP-1881	"When attempting to add vertices and edges with custom IDs using the following traversal, Python returns an error stating that the returned object from the id built-in function is not serializable."	1. Crash	none	none	"I believe the cause is due to id also being a Python built-in function.  We might need to address this in the same manner as we've addressed as, in, and, or, is, not, from, and global by adding id_ to that list?"	3.3 Incorrect inputs	" Using T.id works, so that will be a sufficient solution to this."	3.10 Modifying input values	
TIKA-2408	"I got a ZipException when try to extract text from DOCX file
IMHO, POI is right to throw an exception and stop"	1. Crash	none	none	"Caused by: java.util.zip.ZipException: invalid literal/lengths set
 at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:164)
 at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:122)
 at org.apache.poi.openxml4j.util.ZipSecureFile

Without significant work, we can't fix this in POI's DOM parser. "	3.1 Problems in my libraries	"Our new, experimental SAX-based docx parser ignores problems with numbering and extracts text from this document

See: https://issues.apache.org/jira/browse/TIKA-2109?focusedCommentId=15828228&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15828228
for how to configure tika-config.xml"	"2.2 Modifying my settings
3.6 Implementing wrappers"	https://tika.apache.org/1.24/api/org/apache/tika/parser/microsoft/OfficeParserConfig.html#setUseSAXDocxExtractor
TIKA-2405	I got SAXParseException in text extraction from DOCX file 	1. Crash	none	none	"Caused by: java.io.IOException: Unable to parse xml bean
 at org.apache.poi.POIXMLTypeLoader.parse

Without significant work, we can't fix this in POI's DOM parser. "	3.1 Problems in my libraries	The workaround is to use our currently experimental SAX parser. 	3.6 Implementing wrappers	https://tika.apache.org/1.24/api/org/apache/tika/parser/microsoft/OfficeParserConfig.html#setUseSAXDocxExtractor
TIKA-2404	I got an XMLException when try to extract text from DOCX file	1. Crash	none	none	"at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
 ... 16 more
Caused by: org.apache.poi.POIXMLException

Without significant work, we can't fix this in POI's DOM parser. "	3.1 Problems in my libraries	The workaround is to use our currently experimental SAX parser. 	3.6 Implementing wrappers	https://tika.apache.org/1.24/api/org/apache/tika/parser/microsoft/OfficeParserConfig.html#setUseSAXDocxExtractor
TIKA-2201	"The following document, which is not particularly big, causes an OOM in Tika parser"	1. Crash	TIKA-2210	resolved	"Without significant work, we can't fix this in POI's DOM parser."	3.1 Problems in my libraries	The workaround is to use our currently experimental SAX parser. 	3.6 Implementing wrappers	https://tika.apache.org/1.24/api/org/apache/tika/parser/microsoft/OfficeParserConfig.html#setUseSAXDocxExtractor
THRIFT-4044	Every Travis CI build job has been failing for weeks 	3. Build and testing error	none	none	"Downloading hspec-core-2.4.0...
Configuring hspec-core-2.4.0...
Building hspec-core-2.4.0...
Preprocessing library hspec-core-2.4.0...

The root cause will be tracked here:

https://github.com/hspec/hspec/issues/306

I believe this is a bug in hspec-core 2.4.0."	3.1 Problems in my libraries	We are limiting hspec-core to a version below 2.4.0 until that team resolves the issue.	3.2 Switching to older versions	
THRIFT-3965	VisualStudio project cannot build compiler CPP project	3. Build and testing error	THRIFT-3973	open	"The project's t_generator.h want to use the header file version.h
The file does not exist but version.h.in is there and looks to be expected by the CMake build's CMakeLists.txt"	4.3 Problems in languages	As a workaround I just copied version.h.in to version.h.	2.3 Modifying build files or options	
THRIFT-3358	Makefile:1362: *** missing separator. Stop.	3. Build and testing error	none	none	Recommend you use the cmake build environment to resolve.	4.4 Incorrect techniques	"I just tried building Thrift and got an error

Haven't seen the issue in quite some time (and it hasn't been updated). Recommend you use the cmake build environment to resolve."	2.3 Modifying build files or options	
SYSTEMML-1071	Univar-Stats.dml fails for 8G medium scenario (runUnivar-Stats_A_1M) with below stack trace. Note smaller and larger (80G) scenarios completed without error.	1. Crash	SYSTEMML-1044	closed	It also correctly reported the dimension mismatch due to incomplete data to be addressed in SYSTEMML-1044.	2.5 Borderline cases	"To workaround test data generation issue SYSTEMML-1044, used MR to successfully create 8GB data:

./genDescriptiveStatisticsData_1M.sh gwperftest MR &>> logs/genStatsData.out

Univar-Stats.dml completed successfully (no dimension mismatch) against the complete data set:"	4.3 Switching to other techniques	
SYNAPSE-1098	"The only issue is with the Content-Type in the HTTP header. I am able to set the header to ""application/multipart-related"" but unable to get the boundary added to the content-type. "	2. Unexpected behavior	none	none		3.2 Problems in my clients	Resolved this by adding custom content type to the HTTP header in the custom mediator class.	3.9 Modifying API calls	
SSHD-754	Starting from Putty plink 0.68 (also includes plink 0.69) we started to have OOM errors. 	1. Crash	none	none	"It looks like starting from April 2016, plink enables 'simple' mode for SSH.

Starting from 0.68 it sends 2GB (0x7fffffff) as receive window size. That makes SSHD library to be vulnerable for OOM."	3.2 Problems in my clients	"// The workaround for VCS-797
    // and https://issues.apache.org/jira/browse/SSHD-754
    // the trick is to block writer thread once there are more
    // than 100 messages in either rekey wait queue or nio write queue

Added capability to register a ChannelStreamPacketWriterResolver through which one can wrap the channel inside one's own code that can do throttling. See also (experimental) ThrottlingPacketWriter example in sshd-contrib module."	2.4 Repairing as technical debts	
SPOT-26	DNS Packets with dns.flags.rcode=1 cause ml_ops.sh to crash	1. Crash	none	none	DNS Packets with dns.flags.rcode=1 cause ml_ops.sh to crash	2.5 Borderline cases	"The workaround is to exclude packets with that rcode (via appending ""and dns.flags.rcode != 1"" to process_opt in ingest_conf.json), but this is not desirable"	2.2 Modifying my settings	
SPOT-200	" it seems that when running the spark-submit task from worker.py in spot-ingest/pipelines/proxy/worker.py the spark runner cannot find the kafka/common/TopicAndPartition class, so then begins failing in a massive way (26k lines of errors as each of the threads die)"	1. Crash	SPOT-206		"This appears to have been an issue with the fact that I was running on the version of Spark (1.6) which is listed in the documentation. After reading another bug stating that you are now working on Spark2, I rebuilt the cluster (albeit in HDP rather than CDH) and now the issue no longer occurs."	"3.1 Problems in my libraries
1.2 Fixed by newer versions"	"This appears to have been an issue with the fact that I was running on the version of Spark (1.6) which is listed in the documentation. After reading another bug stating that you are now working on Spark2, I rebuilt the cluster (albeit in HDP rather than CDH) and now the issue no longer occurs."	3.3 Switching to newer versions	
SPARK-22711	When I submit a Pyspark program with spark-submit command this error is thrown.	1. Crash	none	open	"Yes, normally you would not need to import inside the functions.  I think it is because the wordnet module initializes lazily, so when cloudpickle tries to pickle it, it is not complete and something is missed"	3.2 Problems in my clients	"as a workaround if you import wordnet in your function, it seems to work fine"	3.9 Modifying API calls	
SPARK-20394	"I have set dfs.replication is equal to ""1"" in hiveContext object. But It did not work properly. That is replica is 1 for 80 % of the generated parquet files on HDFS and default replica 3 is for remaining 20 % of parquet files in HDFS. I am not sure why the replica is not reflecting to all the generated parquet files."	2. Unexpected behavior	none	none	"I have set dfs.replication is equal to ""1"" in hiveContext object. But It did not work properly. 

IIRC Spark 1.6 doesn't propagate the HiveContext configuration to the Hive library in some cases."	2.7 Problems in settings	"Have you tried setting the replication to 1 in your hdfs-site.xml?

I have edited the hdfs-site.xml for replication value change and override the existing Spark environment. It works now."	2.2 Modifying my settings	
SPARK-14203	"Error in query: Columns of grouping_id (i_class#170) does not match grouping columns (i_category#172,i_class#170);"	2. Unexpected behavior	none	none	Error in query: Columns of grouping_id (i_class#170) does not match grouping columns	3.2 Problems in my clients	grouping(i_class) works..	3.9 Modifying API calls	
SPARK-13863	Query 66 returns wrong results compared to official result set	2. Unexpected behavior	SPARK-13861	closed	"Please check if the table definition is different.

Thus, the accuracy could be caused by the wrong table definition"	3.2 Problems in my clients	"after I modified the DDL to use ""decimal(7,2)"" for the ""double"" colums as documented in the tpc-ds specs and the query return the following results both from Hive and Spark SQL"	3.9 Modifying API calls	
SPARK-13612	Multiplication of BigDecimal columns not working as expected	2. Unexpected behavior	none	none	"Because the internal type for BigDecimal would be Decimal(38, 18) by default, (you can print the schema of x and y), the result scale of x(""a"") * y(""b"") will be 18 + 18 = 36. That is detected to have overflow so you get a null value back."	3.2 Problems in my clients	As of now we are using the below workaround	3.9 Modifying API calls	
SPARK-13246	I noticed that a job reading avro files would have some tasks that never finish. Looking at the threads they got stuck in	"4. Hang

"	AVRO-1773	closed	"The issue is that Schema.parse is not thread safe, and i have multiple tasks calling this method in the same executor.
See here:
https://issues.apache.org/jira/browse/AVRO-1773"	3.1 Problems in my libraries	Workaround is build spark with hadoop included (not provided).	2.3 Modifying build files or options	
SPARK-11516	Spark application cannot be found from JSON API even though it exists	2. Unexpected behavior	none	none	"any JSON API calls other than the /api/v1/applications call require eventLogging to be enabled

Note that this information is only available for the duration of the application by default. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage."	2.7 Problems in settings	Closing as Workaround since turning on eventLogging fixes this	2.2 Modifying my settings	https://spark.apache.org/docs/latest/monitoring.html
SOLR-9958	"The FileSystem used by HdfsBackupRepository gets closed before the backup completes.

My shards get backed up correctly, but then it fails when backing up the state from ZK. "	1. Crash	SOLR-11335	resolved	"it looks like something else closed FileSystem from out under the repo
This may just be a bug in the underlying Google cloud storage impl"	4.1 Problems in operating systems	"Short term fix is to make sure that ""fs.SCHEME.impl.disable.cache=true"" for whatever filesystem you are using if it is NOT HDFS.

SOLR-11473 would be the long term fix"	1.1 Fixed problems	
SOLR-9862	"When i start Solr, I get a message that does not allow Solr to startup"	1. Crash	none	none	"OS: Solaris
JAVA: 1.8.0_111

I think this is a Solaris problem"	4.1 Problems in operating systems	"set the JAVA_VER=8 seeing that the debug information says that Solr picks up the right version
"	4.1 Modifying operating systems	
SOLR-9643	"Either value of ngroups in grouped query is inaccurate or there is some issue in returning documents of later pages.

select?q=:&group=true&group.field=family&group.ngroups=true&start=0&group.limit=1

For above mentioned query i get ngroups = 396324
but for the same query when i modify start to 396320. it returns 0 docs, an empty page."	2. Unexpected behavior	none	none	"you don't mention whether or not your collection is sharded, if it is sharded then the Distributed Result Grouping Caveats might apply to the behaviour you observe i.e.

... group.ngroups and group.facet require that all documents in each group must be co-located on the same shard in order for accurate counts to be returned. ..."	4.1 Problems in operating systems	"guess i will have to co-locate common group results on the one shard.
Let's consider why co-locating documents with the same group works.
The easiest way to co-locate is to have all documents on one shard:"	4.1 Modifying operating systems	https://lucene.apache.org/solr/guide/8_3/result-grouping.html#ResultGrouping-DistributedResultGroupingCaveats
SOLR-9448	"[subquery] calls another collection fails with ""undefined field"" or NPE from mergeIds"	1. Crash	none	none	"In a rare case when a collection under subquery has no unqueKey at all, it leads to something like ""undefined field""

pass unnecessary query parsing.

Don't we need to bypass query parsing in QueryComponent.prepare() when it's followed by distributed processing? "	2.5 Borderline cases	"SOLR-9448.patch proposes a workaround (when a collection for subquery has a different uniqueKey):

add field alias for expected uniqueKey: subq.fl=id:subq_coll_id
add subq.distrib.singlePass=true"	2.2 Modifying my settings	
SOLR-8876	"Morphlines fails with ""No command builder registered for ..."" when using Java 9"	1. Crash	https://github.com/kite-sdk/kite/issues/469	none	"Using wildcards like * and ** in morphline's importCommands config options do not work in java9

Root issue appears to be a (non-test) bug in how morphlines parses it's config file.

This classpath scanning doesn't work in java9 (likely due to the way the ClassLoader specifics have changed)"	4.2 Problems in languages	It appears that this limitation can be worked arround by having a config which explicitly identifies all of the *Builder classes needed for the config	2.2 Modifying my settings	
SOLR-6209	"You can clearly see that there is just only ""Time Elapsed"" parameter increase. ""Total Rows Fetched"" & ""Total Documents Processed"" remains same for infinite time."	"4. Hang

"	none	none	it seems the JDBC driver get stuck in the middle of a read	3.1 Problems in my libraries	"We are connecting to Oracle and our datasoure definition looks like this:

<dataSource name=""jdbc"" driver=""oracle...."" url=""..."" autoCommit=""false"" batchSize=""1000"" transactionIsolation=""TRANSACTION_READ_COMMITTED"" oracle.net.READ_TIMEOUT=""300000"" oracle.jdbc.ReadTimeout=""300000"" />"	3.4 Modifying the settings of libraries	
SOLR-13463	Solr admin user credentials defined with -Dbasicauth property during start is visible in admin UI to any user.	5. Security threat	none	none	"Please use the option to store pw in a file instead, see ref guide."	2.7 Problems in settings	"it worked fine now. Earlier when I was using ""-Dsolr.httpclient.config="" property pointing to basicAuth.conf file in which I defined username:password format which is incorrect. Instead username and password should be in below format to make it working."	2.2 Modifying my settings	https://lucene.apache.org/solr/guide/7_7/solr-control-script-reference.html#enabling-basic-authentication
SOLR-11386	I'm getting some extremely strange behavior when trying to extract features for a learning to rank model. The following query incorrectly says all features have zero values	2. Unexpected behavior	none	none	"According to your solr feature definition and your efi (efi.case_description), that is equivalent to a phrase query

the issue is that the FieldQParser automatically converts multiple terms into phrases"	2.7 Problems in settings	"I believe I've discovered a workaround. If you use:

    {
        ""store"": ""redhat_efi_feature_store"",
        ""name"": ""case_description_issue_tfidf"",
        ""class"": ""org.apache.solr.ltr.feature.SolrFeature"",
        ""params"": {
            ""q"":""{!dismax qf=text_tfidf}${text}""
        }
    }
instead of:

    {
        ""store"": ""redhat_efi_feature_store"",
        ""name"": ""case_description_issue_tfidf"",
        ""class"": ""org.apache.solr.ltr.feature.SolrFeature"",
        ""params"": {
            ""q"": ""{!field f=issue_tfidf}${case_description}""
        }
    }
you can then use single quotes to incorporate multi-term arguments as Alessandro Benedetti suggested."	2.2 Modifying my settings	https://lucene.apache.org/solr/guide/6_6/analyzers.html#Analyzers-AnalysisforMulti-TermExpansion
SOLR-11369	Zookeeper credentials are showed up on the Solr Admin GUI	5. Security threat	none	none	"In Solr 6.6.X, sensitive property redaction was not enabled by default. "	2.7 Problems in settings	"SOLR-10076 fixed this by default in 7.0, to not show system properties that contain ""password"" (case-insensitively). You can modify the system properties to hide by setting system property solr.redaction.system.pattern"	"3.3 Switching to newer versions
2.2 Modifying my settings"	
SOLR-11335	I'm seeing issues where the Hadoop FileSystem instance is closed out from under other objects. 	1. Crash	SOLR-9958	resolved	I'm seeing issues where the Hadoop FileSystem instance is closed out from under other objects. 	4.1 Problems in operating systems	"if I set the fs.$SCHEME.impl.disable.cache to true, then my problems go away,

Marking this as resolved with workaround. SOLR-11473 would be the long term fix. Short term fix is to make sure that ""fs.SCHEME.impl.disable.cache=true"" for whatever filesystem you are using if it is NOT HDFS."	1.1 Fixed problems	
SOLR-11049	Do we need to be worried about the fact that the failed upload took 24 seconds?	6. Performance issue	none	none		1.1 Fixed by related issues	SOLR-11250 in 7.2 adding a DefaultWrapperModel class for loading of large and/or externally stored LTRScoringModel definitions.	3.6 Implementing wrappers	
SENTRY-951	"move hive warehouse dir to /hive, the dir doesn't have hive:hive as owner."	2. Unexpected behavior	none	none	"1. we can do doc to warn user explicitly configure hive:hive to warehouse dir; like /user/hive/warehouse
2. we can change from sentry code to force it to be hive:hive;

Both 1 and 2 are new features, not bug. But if we don't do 1 or 2, there will be a lot of permission issues for hcat, spark, and hive"	3.2 Problems in my clients	"all those changes are inside the patch I posted for the e2e test on mini cluster I added. And step is as you described:
1. mkdir /hive and set the ownership to be hdfs:supergroup;
2. add /hive into sentry prefix:
hadoopConf.set(""sentry.authorization-provider.hdfs-path-prefixes"", ""/user/hive/warehouse,/tmp/external,/hive"");
3. and make it as warehouse dir.
hiveConf.set(""hive.metastore.warehouse.dir"", ""hdfs:///hive"");
4. getAclStatus gets hive:hive."	2.2 Modifying my settings	
RNG-38	Wrong badge on Github	3. Build and testing error	none	none	"The ""README.md"" file should not be re-generated before there is a solution to COMMONSSITE-90 for modular components."	3.1 Problems in my libraries	Badges were manually added.	2.2 Modifying my settings	
RANGER-2149	REST search queries make Ranger incredibly slow	6. Performance issue	none	none	MySQL 5.1 is not recommended for Ranger	"1.2 Fixed by newer versions
3.1 Problems in my libraries"	"MySQL 5.1 is not recommended for Ranger, if possible use 5.6"	3.3 Switching to newer versions	
PLC4X-116	Build fails	3. Build and testing error	none	none	"If the ""with-proxies"" profile is enabled, the build should succeed."	2.7 Problems in settings	"If the ""with-proxies"" profile is enabled, the build should succeed."	2.3 Modifying build files or options	
PDFBOX-4396	 I suspect that G1 is not collecting soft references across all regions before it out-of-memory errors.	1. Crash	PDFBOX-4668	open	I suspect that G1 is not collecting soft references across all regions before it out-of-memory errors.	4.2 Problems in languages	"I think at this point I'll close this, like the other, as not something trivially fixable. I do think better resource handing is warranted, but that requires a thoughtful refactor.

there have not been any related changes."	1.2 Unfixed problems	
OPENNLP-1129	"https://github.com/apache/opennlp/tree/opennlp-1.6.0

Shows zero files and the release zip on github is just an empty file. "	3. Build and testing error	https://github.com/apache/opennlp/tree/opennlp-1.6.0	none	"on the same date (Dec 10, 2014) there is also an opennlp-1.6.0-rc1 tag. I am guessing it was used instead of opennlp-1.6.0"	4.3 Problems in languages	I think it will be ok to close this.	1.2 Unfixed problems	
OPENMEETINGS-1575	Moodle not logging with user soap in OpenMeetings 3.2.0	2. Unexpected behavior	none	none	"Before it works with OpenMeetings 3.1.3

New version of the plugin is required for OM 3.2.x
Plugin is available here: https://github.com/openmeetings/openmeetings-moodle-plugin"	"1.2 Fixed by newer versions
3.1 Problems in my libraries"	"New version of the plugin is required for OM 3.2.x
Plugin is available here:"	3.3 Switching to newer versions	
OPENJPA-2671	Wrong type mapping for double in HSQLDB	2. Unexpected behavior	none	none	"Currently, Java type double is mapped to NUMERIC for HSQLDB
This seems wrong to me, as HSQLDB's NUMERIC is not a binary floating point type, but represents exact decimal numbers and maps to Java type BigDecimal 

I agree that DOUBLE is a better fit, but am a bit frightened that we might introduce a backward incompatibility for where it did work fine.
There is a trick which you can use and we still don't loose backward compatibility for other users

To be honest I'm not quite sure whether we should fix that. The proposed change makes us technically more correct.
But we might break quite a few projects if we change this now. And there is a workaround by manually defining the mapping type."	2.2 Incompatible issues	"There is a trick which you can use and we still don't loose backward compatibility for other users:
You can tweak each DBDictionary in your persistence.xml
Please try the following:
<persistence>
¡­"	2.2 Modifying my settings	https://openjpa.apache.org/builds/3.1.0/apache-openjpa/docs/ref_guide_dbsetup_dbsupport.html
OLINGO-926	Batch Request fails if request line is absolute	2. Unexpected behavior	none	none	"The mentioned URLs relative to the host
GET /ReferenceScenario.svc/Employees HTTP/1.1
are actual not supported by Olingo.

However I agree with you that URLs relative to the host are specified in RFC7230 (in chapter 5.3.1. origin-form) and also allowed in OData V2.

I will check for a fix and give feedback."	2.7 Problems in settings	"with an absolute URL
GET http://localhost:8080/ReferenceScenario.svc/Employees HTTP/1.1
are allowed.

Hi Michael,

I've adjusted my OData client to use the absolute URL as proposed. This seems to work great for all kind of requests"	"4.3 Switching to other techniques
"	
OFBIZ-9664	"Perhaps this is no bug, but a new feature: After migrating to OFBiz 16, content of type ""Long Text"" containing HTML is now displayed in the ecommerce shop frontend with certain attributes deleted, e.g. ""class"" and ""id"". Is there a config file to allow those attributes to be displayed?"	2. Unexpected behavior	none	none	people should be carefull with this workaround. Because it removes some security in all other parts	2.1 Flawed repairs	The solution/workaround for this issue is: Set sanitizer.permissive.policy=true in framework/base/config/owasp.properties	2.2 Modifying my settings	https://ofbiz.apache.org/release-notes-16.11.01.html
OFBIZ-6807	[UtilXml.LocalResolver.resolveEntity] could not find LOCAL DTD/Schema with publicId [null] and the file/resource is [	1. Crash	http://ofbiz.135035.n4.nabble.com/could-not-find-LOCAL-DTD-Schema-with-publicId-null-and-the-file-resource-is-simple-methods-xsd-td163234.html	none	"As it was annoying, waiting for a definitive and complete solution, as a temporary fix I keept only <<web-app version=""3.0"">> in web.xml files instead of whole xmlns and schemaLocation. Done at
trunk r1726388
R15.12 r1726389"	2.1 Flawed repairs	"as a temporary fix I keept only <<web-app version=""3.0"">> in web.xml files instead of whole xmlns and schemaLocation. Done at
trunk r1726388
R15.12 r1726389"	2.2 Modifying my settings	
ODFTOOLKIT-375	running the first line jumps to the finally block directly without giving any kind of error	1. Crash	none	none	"the program works correctly with version odftoolkit-0.5 but not with odftoolkit-0.6 version

Try a new version as 0.6.1 "	1.2 Fixed by newer versions	Try a new version as 0.6.1 	3.3 Switching to newer versions	
ODE-1073	org.apache.openjpa.persistence.PersistenceException	1. Crash	http://openjpa.apache.org/builds/1.2.1/apache-openjpa-1.2.1/docs/javadoc/org/apache/openjpa/persistence/PersistenceException.html	none	"Workaround: Disable Streaming Resultsets for Mysql.

Add this property in ode-axis2.properties file

openjpa.FetchBatchSize=-1"	2.7 Problems in settings	"Workaround: Disable Streaming Resultsets for Mysql.

Add this property in ode-axis2.properties file

openjpa.FetchBatchSize=-1"	2.2 Modifying my settings	
OAK-7779	java.lang.RuntimeException: org.apache.jackrabbit.oak.api.CommitFailedException: OakNamespace0005: Namespace modification not allowed: rep:nsdata	1. Crash	none	none	"OAK-7786 fixed the original issue described here

Solved with OAK-7786"	1.1 Fixed by related issues	Solved with OAK-7786	1.1 Fixed problems	
NIFIREG-142	"First start problem: Error creating bean with name 'flywayInitializer'

java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException"	1. Crash	none	none	"currently there is a requirement that NiFi Registry be run with a JDK and not a JRE, this error happens when it is started with a JRE and Flyway (database migration tool) attempts to use something from the JDK."	3.1 Problems in my libraries	hopefully work since you have the JDK now.	3.1 Switching to other libraries	https://nifi.apache.org/docs/nifi-registry-docs/html/administration-guide.html
NETBEANS-2840	We have a builds failing	3. Build and testing error	NETBEANS-2345	resolved	"As the site is currently down, we're not able to build out our dependencies."	4.3 Problems in languages	 we've found an alternative repo to resolve our dependencies at https://repo.spring.io/plugins-release. 	2.2 Modifying my settings	
NETBEANS-2733	"type psvm + TAB

This should provide a System.out.println(..) but just provides the text psvm + the TAB"	2. Unexpected behavior	none	none	"Observe: This seems to be a problem for ALL the templates defined in 

Tools --> Options --> Code Templates"	2.7 Problems in settings	Deleting the Cache and User Directory solved the problem	4.2 Deleting temporal files	"https://netbeans.org/bugzilla/show_bug.cgi?id=267606

http://wiki.netbeans.org/FaqWhatIsUserdir"
NEMO-416	we have the below ClassCastException	1. Crash	https://github.com/apache/incubator-nemo/pull/237	no merge	"There are two guava dependencies of Nemo

Root culprits relates to guava deserialization internals"	3.1 Problems in my libraries	Work around the problem by	3.7 Bypassing APIs with bugs	
MYFACES-3104	java.lang.VerifyError	1. Crash	none	none	"Sample JSF 2 application that we try to run on Websphere 7.0 with :
MyFaces 2.0.4

Definitively it is a bug in websphere 7.

The MyFaces commiters mentioned that it's more a WebSphere bug."	3.2 Problems in my clients	"You can use org.springframework.web.jsf.el.SpringBeanFacesELResolver as replacement.

Not using org.springframework.web.jsf.el.SpringBeanFacesELResolver was the issue. We were incorrectly using the deprecated org.springframework.web.jsf.DelegatingVariableResolver.

To sum things up, here are snippets of the web.xml and faces-config.xml that were used to get this working...
web.xml:"	"3.4 Modifying the settings of libraries
3.9 Modifying API calls"	
MYFACES-2869	Ajaxified h:selectBooleanCheckbox not working in IE8	2. Unexpected behavior	none	none	"This works fine with FF, Safari and Chrome but not with IE8. The resaon for this is that the default onchange event is not working correctly in IE8."	4.4 Incorrect techniques	"A workaround for this is to set the event to click manually

IE8 requires event=click and the functionality is fine with event=click."	3.9 Modifying API calls	
MRM-1881	Release 2.2.0 does not work with PostgreSQL	1. Crash	none	none	"Release 2.2.0 does not work with PostgreSQL

There a few things that need to be addressed:
Include PostgreSQL ODBC driver in the build

it seems that the problem is related to the defaultTransactionIsolaction which was missing from the config"	"3.1 Problems in my libraries
2.7 Problems in settings"	"So I managed to solve the database configuration by adding:

<Set name=""defaultTransactionIsolation"">8</Set>

Also we would need to include postgresql drivers inside the distro."	"3.4 Modifying the settings of libraries
"	
MRM-1845	Virtual repository not accessible behind https reverse proxy	2. Unexpected behavior	none	none	This header is filled by reverse proxy.	4.1 Problems in operating systems	"Workaround : If an Apache is in front of Archiva, you could cancel this problem with this configuration rule in VirtualHost context (prerequisite : headers_module):

RequestHeader unset Authorization"	4.1 Modifying operating systems	https://archiva.apache.org/docs/2.2.3/release-notes.html
MNG-6417	unable to print any colored text	2. Unexpected behavior	none	none	The source of this trouble is jansi library	3.1 Problems in my libraries	"In jansi website it clearly say that we can pass jansi.passthrough=true to force jansi to print out colored text. I also tried to set the MAVEN_OPTS variable but no help. When i print out the value in jansi code (jansi/jansi/src/main/java/org/fusesource/jansi/AnsiConsole.java), i got a false. I i hard-coded it and force it to get into the jansi.passthrough if-loop. Then i recompile my new maven with my modified jansi library. Finally everything works."	"3.8 Deep copying
2.3 Modifying build files or options
"	
MESOS-7216	"all of the ASSERTS in the test itself pass, but the test failed because there were remaining processes once the test exited "	2. Unexpected behavior	none	none	"I worked around this by setting the executor shutdown grace period to 10secs, a better solution would be to make the I/O Switchboard optional."	2.1 Flawed repairs	I worked around this by setting the executor shutdown grace period to 10secs	2.2 Modifying my settings	
MESOS-6577	"Failed to launch container: Failed to run 'docker -H unix:///var/run/docker.sock inspect mesos-84a9df2b-be0e-459e-afc9-b95d4e8ced57-S0.0116a0a2-ccaf-4f1a-846c-361ec4e4a179': exited with status 1; stderr='Error: No such image, container or task"	1. Crash	none	none	"In your case, (1) is in process, but (2) immediately errors. This means that the docker daemon is not processing the docker run until after the docker inspect has finished. "	2.7 Problems in settings	switching from -executor_registration_timeout=2mins to -executor_registration_timeout=5mins	2.2 Modifying my settings	
MESOS-5672	When building EGG/WHL(s) setuptools will enforce PEP 440 standards which will normalize '1.0.0-rc0' to '1.0.0rc0'	2. Unexpected behavior	none	none	This will be an internal versioning/tagging/labeling issue we'll have to work-around	4.2 Problems in languages	This will be an internal versioning/tagging/labeling issue we'll have to work-around	3.10 Modifying input values	
MESOS-5624	will fail to compile with the following error:	3. Build and testing error	none	none	I/mnt/mesos/mesos/src/src looks odd. Seems like some include paths could be wrongly generated.	2.7 Problems in settings	"Workaround, by out-of-place building:
https://reviews.apache.org/r/48795/"	2.3 Modifying build files or options	
MESOS-5441	Test fixtures inheriting from mesos::internal::tests::ContainerizerTest fail if sudo ./bin/mesos-tests.sh is run.  Here's an example from our internal CI	3. Build and testing error	none	none	"It seems that lxcfs of Ubuntu 16.04 might be causing this

seems that lxcfs also mounts a cgroup hierarchy."	4.1 Problems in operating systems	Disabling the service (systemctl disable lxcfs) resolved this.	4.1 Modifying operating systems	
MESOS-4827	"when we destroy/re-deploy a docker-containerized task, the mesos-slave got killed from time to time. "	1. Crash	https://stackoverflow.com/questions/35713985/destroy-docker-container-from-marathon-kills-mesos-slave	none	"3- Mesos slave is restarted (by systemd), and is trying to reregister in the mesos cluster. It does not detect any checkpointed resources in /tmp/mesos/meta/resources/resources.info, and fails to find the latest slave from '/tmp/mesos/meta'.

3 is what is causing the wide-spread task failure.

it is indeed not configured, so default value is /tmp/mesos. Will change it asap.

we don't see that again after work_dir has been changed."	2.7 Problems in settings	Resolving since we don't see that again after work_dir has been changed.	2.2 Modifying my settings	
MDEPLOY-221	deploy generates wrong timestamps in maven-metadata.xml	2. Unexpected behavior	none	none	"This is indeed a Maven Core 3.5.0 issue

This is fixed in Maven 3.5.1"	1.2 Fixed by newer versions	"I don't see another way to work-around this at the plugin level for 3.5.0, update of Maven is necessary."	3.3 Switching to newer versions	
MCOMPILER-277	Compilation error while compiling for 1.6 using JDK 1.8	3. Build and testing error	none	none	This is not a feature of the plugin	4.4 Incorrect techniques	If you need to support different JDK's there is not really an other way to support that via installing the needed JDK's. Using toolchains is the correct way to handle this in Maven.	4.3 Switching to other techniques	https://maven.apache.org/plugins/maven-compiler-plugin/examples/compile-using-different-jdk.html
MASSEMBLY-828	"art of the assembly is to copy xml files from the project to the output directory. In versions after 2.1, maven-assmbly-plugin is dropping some attributes that are defined in the xml as it copies it to the output directory. "	2. Unexpected behavior	none	none	"In version 2.1 of the assembly plugin it did this. In versions above that, the affect of copying the second fileset does not "	2.4 Recurring problems	I was able to resolve this by using the <excludes> option for the file set in the assembly descriptor.	3.11 Modifying input format	
MADLIB-889	could not write to temporary file: No space left on device	2. Unexpected behavior	none	none	association rules limitiaon	2.7 Problems in settings	this is addressed with max_itemset_size param	2.2 Modifying my settings	https://madlib.apache.org/docs/latest/group__grp__assoc__rules.html
LUCENE-7248	Interrupting IndexWriter causing unhandled ClosedChannelException	1. Crash	none	none	The problem is not in Lucene it is in the software that sends interrupts to threads by Lucene. 	3.2 Problems in my clients	"If you cannot fix this, your only chance is to use RAFDirectory, but this slows down reading from index.

Using RAFDirectory did not solve the problem."	1.2 Unfixed problems	
LOG4J2-2281	It even crashes the server sometimes	1. Crash	none	none	"Ok this seems to be a bug in WebSphere's annotation scan algorithm.
com.ibm.ws.ecs.internal.scan.context.impl.ScannerContextImpl
scanJAR unable to open input stream for resource
META-INF/versions/9/org/apache/logging/log4j/util/ProcessIdUtil.
class in archive WEB-INF/lib/log4j-api-2.9.1.jar"	3.2 Problems in my clients	The issue has been fixed in WebSphere 8.5.5.14 or 9.0.0.7	1.1 Fixed problems	
LIVY-49	The Spark session could not be created in the cluster:	1. Crash	none	none	this turns out to be a Spark bug	3.1 Problems in my libraries	Filed SPARK-13478 for the Spark fix. 	1.1 Fixed problems	
LENS-1009	After some time the logs get cluttered since purge failure logs are everywhere.	7. Warning-style error	none	none	"Solution is to change the type of query column to text in underling DB

If a query is larger than that, while purging it throws an exception regarding data truncation. After that, there are retries for purging and it keeps on retrying. "	2.1 Flawed repairs	Solution is to change the type of query column to text in underling DB	3.4 Modifying the settings of libraries	
KYLIN-4123	ClassCastException when performing Integration Tests	1. Crash	KYLIN-4040	CLOSED	"Hortonworks HDP 3.0.1.0-187

you may try to use HDP 2.4.0.0-169"	"3.1 Problems in my libraries
2.4 Recurring problems"	you may try to use HDP 2.4.0.0-169	3.2 Switching to older versions	http://kylin.apache.org/development/dev_env.html
KYLIN-4082	Throw class not found exception when start kylin	1. Crash	none	none	pache-kylin-2.6.3-bin-cdh60.tar.gz and cdh6.2 both don't contain the jar file.	4.3 Problems in languages	When download commons-configuration-1.6.jar from the internet and put it into $KYLIN_HOME/lib£¬kylin finally starts successfully	2.3 Modifying build files or options	
KYLIN-3751	It is a NullPointerException in VolcanoPlanner	1. Crash	none	none	you can upgrade your kylin to v2.5.2 directly.	1.2 Fixed by newer versions	you can upgrade your kylin to v2.5.2 directly.	3.3 Switching to newer versions	http://kylin.apache.org/docs/release_notes.html
KYLIN-2587	"""Convert Cuboid Data to HFile"" failed on EMR 5.5"	1. Crash	none	none	but the reducer couldn't get zookeeper's info	3.1 Problems in my libraries	"I bypassed this error by copy ""hbase.zookeeper.quorum"" from hbase-site.xml to $KYLIN_HOME/conf/kylin_job_conf.xml:

<property>
<name>hbase.zookeeper.quorum</name>
<value>ip-nn-nn-nn-nn.ap-northeast-2.compute.internal</value>
</property>"	2.2 Modifying my settings	http://kylin.apache.org/docs/install/kylin_aws_emr.html
KYLIN-2326	ERROR: ArrayIndexOutOfBoundsException: -1	1. Crash	KYLIN-1971	CLOSED	The problem is when you have same name column in Dim and Fact Table	2.7 Problems in settings	Will be solved in Kylin 2.0.0	3.3 Switching to newer versions	
KYLIN-2020	Failed to build sample cube on a EMR 5.0 client node	3. Build and testing error	none	none	"But in some Hadoop distribution (like EMR 5.0), the hbase shell doesn¡¯t keep the origin HBASE_CLASSPATH value, that causes the ¡°NoClassDefFoundError¡±."	3.1 Problems in my libraries	"To fix this, find the hbase shell script (in hbase/bin or hbase/conf folder), and search HBASE_CLASSPATH, check whether it overwrite the value like :"	3.4 Modifying the settings of libraries	https://kylin.apache.org/docs15/gettingstarted/faq.html
KYLIN-1842	java.lang.NoClassDefFoundError: org/apache/hive/hcatalog/mapreduce/HCatInputFormat	1. Crash	none	none	"But in some Hadoop distribution (like EMR 5.0), the hbase shell doesn¡¯t keep the origin HBASE_CLASSPATH value, that causes the ¡°NoClassDefFoundError¡±."	3.1 Problems in my libraries	"To fix this, find the hbase shell script (in hbase/bin or hbase/conf folder), and search HBASE_CLASSPATH, check whether it overwrite the value like :"	3.4 Modifying the settings of libraries	https://kylin.apache.org/docs15/gettingstarted/faq.html
KYLIN-1089	Kylin failed to run on CDH with HBase 1.0	1. Crash	none	none	"The root cause is, CDH reverts several Apache HBase 1.0 commits, for backward compatibility; While Kylin 1.x-HBase1.x is compiled with Apache HBase 1.1 API, which is not compitible with CDH"	3.1 Problems in my libraries	Branch for CDH 5.7	3.3 Switching to newer versions	
KUDU-402	I've seen a couple cases on Jenkins where linked_list-test hangs	"4. Hang

"	none	none	"Thankfully, the bug is fixed upstream in clang, and will be in the 3.5 release. "	"3.1 Problems in my libraries
1.2 Fixed by newer versions"	"Thankfully, the bug is fixed upstream in clang, and will be in the 3.5 release. It also seems to be pretty rare, so we'll ignore it for now."	3.3 Switching to newer versions	
KUDU-282	Test when errors occur on the Apply() phase of transactions	1. Crash	none	none	"we ended up making IO errors fatal, so I don't think there are any errors that could happen on Apply anymore."	2.7 Problems in settings	"we ended up making IO errors fatal, so I don't think there are any errors that could happen on Apply anymore."	2.2 Modifying my settings	
KUDU-1419	Kudu may fail to start in docker when using Ubuntu/AUFS	1. Crash	none	none	"By default Ubuntu's docker setup uses AUFS for its storage layer. That leads to problems during startup because rename() may not work in AUFS.

The problem is that we rely on this atomic directory rename for durability"	4.1 Problems in operating systems	"based on a conversation on Slack, Dan and Todd suggested we should just work around this by requiring Docker users to mount an external volume for their Kudu data. 

Resolved as ""workaround"" since people can use a volume or another filesystem."	4.1 Modifying operating systems	
KAFKA-5861	application is hanging in some streaming chaos.	"4. Hang

"	none	none	From this stack I saw that indeed System.exit() blocks complete process.	3.2 Problems in my clients	"From this stack I saw that indeed System.exit() blocks complete process.
So finally I modify my shutdown hook handler to such a code"	3.9 Modifying API calls	
KAFKA-4149	 in the Jenkins jobs for trunk (jdk7 and jdk8) and PRs.	3. Build and testing error	none	none	The root cause must have been a failure in building the right class path when running the tests.	2.7 Problems in settings	The problem seems to go away if the `jarAll` task before `testAll` is removed	2.3 Modifying build files or options	
JEXL-259	the following scripts are terminated with error unsolvable property 'c'	1. Crash	none	none	"This is the intended behavior;

As is, this issue is not a bug; as an improvement, it might be possible to detect/flag 'pure' antish references and set/get them through overridable method."	2.2 Incompatible issues	"I ditched the ant-ish variables and have managed to write cascade resolver that resolves one part after another. That was not a generic solution for all cases, but was just enough for my use-case."	4.3 Switching to other techniques	
JENA-1207	A TDBException is thrown.	1. Crash	none	none	"Use
RDFDatatype datatype = XSDDatatype.XSDdecimal ;
rather than new XSDDatatype(""decimal"");."	3.2 Problems in my clients	"Use
RDFDatatype datatype = XSDDatatype.XSDdecimal ;
rather than new XSDDatatype(""decimal"");."	3.9 Modifying API calls	
JCLOUDS-487	NodeMetadata.getPrivateAddresses() returns a list of addresses in random order. These should be returned in a particular order (as listed by the service)	2. Unexpected behavior	none	none	here is some neutron code that can accomplish figuring out what IPs belong to what network ids:	3.2 Problems in my clients	"Alright, here is some neutron code that can accomplish figuring out what IPs belong to what network ids:"	3.9 Modifying API calls	
JCLOUDS-1326	Peak memory used before it crashes is ~15GB.	1. Crash	none	none	It appears that jclouds is logging the entire payload ¨C how have you configured logging?	2.7 Problems in settings	"As per this example I tried, yes, it is the wire logs causing issue. Switching off wire logs helped to get past OOM issue. "	2.2 Modifying my settings	
JCLOUDS-1293	java.util.concurrent.ExecutionException: org.jclouds.http.HttpResponseException: Unexpected end of file	1. Crash	none	none	"The PR with the fix to mitigate the issue is now merged. The cause of the failure, an interruption to the connection at the underlying http/network level, does not seem caused or related to jclouds, and the most we can do is our best to retry when that happens. "	4.5 Problems in the network	"Another workaround that would be safer is to create a custom retry handler 

The PR with the fix to mitigate the issue is now merged. The cause of the failure, an interruption to the connection at the underlying http/network level, does not seem caused or related to jclouds, and the most we can do is our best to retry when that happens. 

https://github.com/apache/jclouds/commit/5113be22d888ba2bd403ef7549ea692163f5ff10"	2.4 Repairing as technical debts	
JCLOUDS-1169	Basically the code crashes	1. Crash	none	none	"we upraded Gson to 2.7 in the latest snapshot, but that breaks OSGi, as some internal packages of Gson that jclouds uses are no longer exported. 

yes
I am using the version 2.6.2."	"3.1 Problems in my libraries
2.4 Recurring problems"	"Upgrade to Gson 2.5 instead of Gson 2.7

Changed the GSON version of my app to match the one with jclouds.
It is working for now."	3.2 Switching to older versions	
ISIS-1963	java.lang.IllegalArgumentException	1. Crash	none	none	Possibly related to the latest Isis upgrade using Wicket 8 instead of 7.9.	3.1 Problems in my libraries	So a quickfix on our side could be to simply catch these IllegalArgumentExceptions and ignore them.	"3.7 Bypassing APIs with bugs
"	
INFRA-18150	our internal (to Adobe) Jenkins infrastructure consistently fails to checkout from svn since March 24th	3. Build and testing error	none	none	the IP was banned for abuse on repository.apache.org 10 days ago.	4.3 Problems in languages	that temporary solution (unblock the IP but keep it banned for repository.apache.org) looks very good to me. 	2.2 Modifying my settings	
INFRA-16907	"When trying to add a ""Slaves"" axis, the Jenkins web UI throws JavaScript errors in both Chrome and Firefox browsers. "	3. Build and testing error	none	none	"This is a bug in the Jenkins software itself, nothing Infra can do here other wait for a future release to fix it."	3.2 Problems in my clients	"Workaround is to ignore 'Slave' axis and use a 'Label Expression' axis instead , see this jobs config for details."	2.2 Modifying my settings	
INFRA-15981	gitbox does not handle 404s well	3. Build and testing error	none	none	"It's a bug in gitweb

This will eventually have to make it to upstream for this to stick."	3.1 Problems in my libraries	I've implemented a workaround to stop the double blurb. This will eventually have to make it to upstream for this to stick. 	3.8 Deep copying	
INFRA-15934	Latest commits are not showing up on project website	3. Build and testing error	none	none	unknown	4.3 Problems in languages	"The gitwcsub daemon appears to have stopped.
I've bounced it and am monitoring if something bad happens in the business logic. "	1.1 Fixed problems	
INFRA-15687	Unable to submit	3. Build and testing error	none	none	"This is a known bug with certain extensions, specifically the Lastpass extension. "	4.4 Incorrect techniques	"Environment:Firefox 57.0.1 on Ubuntu 16.04. Same thing with Chrome.

Can workaround by disabling Lastpass. "	4.3 Switching to other techniques	
INFRA-15128	"Travis tries to build (fails) on invalid branch ""0.7.0"" which no longer exists"	3. Build and testing error	none	none	"Hello Chris,

Unfortunately, we cannot delete or hide branches at this time. It's in our backlog to give the ability to hide them in the future but we do not have an ETA on when this would be possible. Sorry again for the inconvenience."	2.2 Incompatible issues	" We decided to live with the invalid ""active branch"" in Travis¡­"	1.2 Unfixed problems	
INFRA-14971	Confluence has lost all code examples in Tapestry docs	3. Build and testing error	none	none	"I believe the source code examples are all now appearing correctly. However, this means our docs can't be edited with Confluence until this issue is resolved.

It's been 4 months and this is still a severe problem for us. The cwiki-test instance looked promising back in September. Is there a technical obstacle remaining, or is it just a matter of priorities and schedules?

I think I could have used the space export/import process to do the same, but maybe there would have been problems from that.
In any case, for Tapestry the issue is resolved and this ticket can be closed."	2.1 Flawed repairs	"I have completed the manual process of correcting all of the Tapestry pages in Confluence by copying their wiki content from the cwiki-test instance.

In any case, for Tapestry the issue is resolved and this ticket can be closed."	2.4 Repairing as technical debts	
INFRA-14922	All the messages in Sep 2002 for committers@ are duplicated.	3. Build and testing error	none	none	the ASF copy wasn't conforming to specs.	2.7 Problems in settings	"I'll have to defer to [~gstein] on this. I cannot share this information with you, let alone in a public JIRA ticket."	3.8 Deep copying	
INFRA-13203	The page https://www.apache.org/dev/writable-git still refers to writable git as an experiment	2. Unexpected behavior	none	none	The page https://www.apache.org/dev/writable-git still refers to writable git as an experiment	3.3 Incorrect inputs	I have resolved this by updating the text on that page.	2.1 Modifying documents	
INFRA-13145	We have a PMC member (skolbachev) who unsuccessfully tried to subscribe to the private Cayenne list for some time.	2. Unexpected behavior	none	none	"As for the root cause, i suspect your MAIL FROM: envelope header in SMTP might differ from what you are subscribed with as a moderator on the list"	3.3 Incorrect inputs	"I've subbed skolbachev to the list now, so this ticket could be closed."	1.1 Fixed problems	
INFRA-12664	"so the site plugin can perform a svn commit as part of the build

"	3. Build and testing error	none	none	"there is no ""role account"" available for checking into SVN. However, according to INFRA-12231, there is such an account for Git."	4.4 Incorrect techniques	Of course switching the site to git would be great 	4.3 Switching to other techniques	
INFRA-12589	their attempt to email java-user-owner@lucene resulted in Apache's qmail-send bouncing their email 	2. Unexpected behavior	none	none	"when ezmlm tried forwarding the email to my (gmail powered) email address google's SMTP server rejected it due to the DMARC policy of the original senders domain.

we'll update our setup with DKIM/DMARC"	2.7 Problems in settings	we'll update our setup with DKIM/DMARC	2.2 Modifying my settings	
INFRA-12559	Intermittent DNS errors on whimsy3	7. Warning-style error	none	none	"I confirm the issue on whimsy-vm3

This is a problem on the remote end. "	4.1 Problems in operating systems	This is a problem on the remote end. The mirror requestor has changed their hostname to resolve an unknown issue on their end.	4.1 Modifying operating systems	
INFRA-11589	"the link does not work; it shows just:

""No such document html"""	2. Unexpected behavior	none	none	"paste.a.o web files appear to be corrupt

""No such document html"""	4.3 Problems in languages	"I've removed references to the about page for now, as we don't have such a page."	2.1 Modifying documents	
IMPALA-6218	Crash due to memory allocation failure on secure cluster	1. Crash	none	none	this was actually due to the setting of /proc/sys/vm/overcommit_memory	4.1 Problems in operating systems	Correctly setting overcommit_memory=1 makes this issue go away.	4.1 Modifying operating systems	https://impala.apache.org/docs/build/impala-2.12.pdf
IMPALA-4843	Full outer join behaves like left outer join	2. Unexpected behavior	none	none	"The problem reproduce scenario is this;
1- The left table is a very large table, right table is a relatively small table"	2.5 Borderline cases	"There's a good chance the bug has been fixed in a more recent version.

This is very likely fixed in a more recent version. "	3.3 Switching to newer versions	
IMPALA-3341	Python environment error breaks ASAN build	3. Build and testing error	none	none	"the problem is the run is trying to use py test installed on the system instead of py test from the virtualenv. 

I've seen things like this happen when we make changes to our python virtual env. If there is still ""stuff"" left in the same directory from an older run of the same job, then python sometimes gets confused."	4.2 Problems in languages	"Wiping the workspace usually solves these kinds of problems. I'll wipe the workspace and restart the job.

What seems to work is to do a ""clean"" build. I checked the ""clean"" checkbox for this job and it passed."	4.2 Deleting temporal files	
IMPALA-3248	Many packaging builds failed	3. Build and testing error	none	none	"The system packaging tools died complaining that ""../../../toolchain/kudu-0.7.0/lib64/libkudu_client.so.0.1.0"" was not a valid library."	3.1 Problems in my libraries	"Moving the toolchain out of the source tree fixes the problem.

When the toolchain is in the source tree, the build command includes the lib with a relative path

 g++ <more stuff> ../../../toolchain/kudu-0.7.0/lib64/libkudu_client.so.0.1.0 <more stuff>"	2.3 Modifying build files or options	
IMPALA-2655	Catalog throws NPE	1. Crash	none	none	"This is a bug in the Java krb5 library

Closing this as a workaround will be addressed by IMPALA-5129."	"3.1 Problems in my libraries
1.1 Fixed by related issues"	Closing this as a workaround will be addressed by IMPALA-5129.	1.1 Fixed problems	
IMPALA-2511	The following build started 4 days ago and is still not finished 4 days later	"4. Hang

"	none	none		2.3 Flaky problems	 why don't you just restart the job to see if the problem persists?	1.1 Fixed problems	
IGNITE-5505	AffinityKeyMapped annotation is ignored if class names are configured on BinaryConfiguration	2. Unexpected behavior	IGNITE-5795	resolved	I discussed this with Vladimir Ozerov again and confirmed we do not want to fix this since any fix would violate Ignite design ideology.	2.2 Incompatible issues 	"To work around the problem in addition to specifying affinity key field using @AffinityKeyMapped annotation also specify it in the cache key configuration like this:

<property name=""cacheConfiguration"">"	2.2 Modifying my settings	
IGNITE-11401	Current lambdas for feature extraction doesn't work in binary builds and fail with such errors:	1. Crash	none	none	The permanent solution will be provided by the linked ticket.	1.1 Fixed by related issues	Replace current lambdas with fixed API	3.9 Modifying API calls	
HIVEMALL-30	continuous-integration/travis-ci/pr throw OutOfMemoryError	1. Crash	none	none	root cause was due to Spark memory requirements.	3.2 Problems in my clients	All build Jobs will succeed if increase maximum Java heap size to 1536m	2.2 Modifying my settings	
HIVE-21367	Hive returns an incorrect result when using a simple select query	2. Unexpected behavior	none	none	Setting property hive.fetch.task.conversion= to none resolved the issue.	2.7 Problems in settings	Setting property hive.fetch.task.conversion= to none resolved the issue.	2.2 Modifying my settings	
HIVE-18849	Caused by: java.lang.NullPointerException	1. Crash	none	none	"
jdk 9 support is not there yet"	4.2 Problems in languages	"After adjusting a few details to complete switch to Java 9 had to modify parent pom.xml.

add dependency to javax.annotation-api
switch to javac compiler"	2.3 Modifying build files or options	
HDFS-9513	java.lang.ArrayIndexOutOfBoundsException	1. Crash	none	none	"We is upgraded our new HDFS cluster to 2.7.1,but we YARN cluster is 2.2.0

but HDFS support storage type feature from 2.3.0 HDFS-2832, older version not have storageId

Patch provided might work as workaround for older clients."	3.2 Problems in my clients	I have few comments about the patch.	3.6 Implementing wrappers	
HDFS-13139	 decommissioning fails with below given error:	2. Unexpected behavior	none	none	"Our default configuration to store data on datanode ""dfs.datanode.data.dir"" was pointing to a drive with lowest capacity (around 3 % of overall DN storage). This 3 % < 3.5 % had made HDFS capacity as 0%"	2.7 Problems in settings	"To fix the downscaling issue, either, we need to lower down non hdfs reserved capacity (lower than 3 %) or point our datanode to higher disk capacity (greater than 3.5 %)."	2.2 Modifying my settings	
HAWQ-1300	hawq cannot compile with Bison 3.x.	3. Build and testing error	none	none	"I use Bison 3.0.2. And looks actually like a bug in gram.c source for this
> Bison version.

This is not HAWQ bug, it is Bison 3.x bug."	3.1 Problems in my libraries	The workaround for HAWQ users is to un-install Bison 3.x and install version 2.7.	3.2 Switching to older versions	
HADOOP-14177	 not english characters will show incorrect and may fail the build.	3. Build and testing error	none	none	A locale isn't set in the Dockerfile	4.1 Problems in operating systems	"start-build-env.sh:
docker build t ""hadoop-build${user_id}"" - UserSpecificDocker
¡­
¡­
RUN locale-gen en_US.UTF-8
ENV LANG='en_US.UTF-8' LANGUAGE='en_US:en' LC_ALL='en_US.UTF-8'"	2.3 Modifying build files or options	
GROOVY-8041	The templating engine does not handle properly the binding parameters that contains dot ('.') character	2. Unexpected behavior	none	none	you will have to work with a patched groovy version and use a custom groovy version for your application. Because there is no chance of fixing this in a normal groovy release	2.2 Incompatible issues	you will have to work with a patched groovy version and use a custom groovy version for your application. Because there is no chance of fixing this in a normal groovy release	3.8 Deep copying	http://docs.groovy-lang.org/latest/html/gapi/groovy/util/ConfigSlurper.html
GEODE-6740	This resulted in the error starting up the locators	1. Crash	none	none	"The membership service prefers to use numeric host addresses in order to avoid using DNS, which can hang if there is a network failure.  Use of host names can be forced with the system property

-Dgemfire.forceDnsUse=true."	4.5 Problems in the network	"It looks like `-Dgemfire.forceDnsUse=true` only affects hostname resolution in the InternalDistributedMember.  On a closer look, it appears the IP address which needs reverse lookup is coming from a ServerLocation object which is part of profile creation, and it is not affected by this flag.  I'll post more details about this as I find them."	2.4 Repairing as technical debts	
GEODE-5410	Throw KeyNotFoundException occurs when using MSTest with NativeClient	1. Crash	none	none	try disabling app domains in XUnit.	4.4 Incorrect techniques	try disabling app domains in XUnit.	3.4 Modifying the settings of libraries	
GEODE-2412	Builds are failing in pipeline due to SSL locator tests failing.	3. Build and testing error	GEODE-1793	REOPENED	"I haven't figured out the
cause yet. Ignore it until I have time to fix it properly."	2.3 Flaky problems	"Ignore the test for now.
Closing this ticket, reopened the original to find a proper fix."	1.2 Unfixed problems	
GEODE-2151	"I was getting the following error while running the unit tests:

org.apache.geode.internal.cache.partitioned.PartitionedRegionLoadModelJUnitTest > testRedundancySatisfactionPreferRemoteIp FAILED
    java.lang.AssertionError:"	3. Build and testing error	none	none	the workaround can cause issues with local servers not finding your canonical hostname.	2.1 Flawed repairs	"My `/etc/hosts` file had the following line:

127.0.0.1 localhost gosullivan-mbpro
Changing it to this fixed the problem:

127.0.0.1 localhost
255.255.255.255 broadcasthost
::1             localhost"	4.1 Modifying operating systems	https://geode.apache.org/docs/guide/110/tools_modules/gfsh/configuring_gfsh.html
FREEMARKER-87	"I'm using struts2, use FriendlyMapModel instead of default MapModel, It works fine except map have key such as ""size"", the value will be unexpected SimpleMethodModel"	2. Unexpected behavior	none	none	" Just don't use it. It can't be fixed, as that breaks backward compatibility."	2.2 Incompatible issues	I recommend extending one of those	3.5 Overridden APIs	https://javadoc.io/static/org.apache.struts/struts2-core/2.1.8.1/org/apache/struts2/views/freemarker/StrutsBeanWrapper.FriendlyMapModel.html
FOP-2726	 getting NullPointerException	1. Crash	none	none	we analysed and found to be an issue with this file: ${JAVA_HOME}/jre/lib/cmm/sRGB.pf file.	3.1 Problems in my libraries	I solved this issue on tomcat server by specifying the lookup path for these files via the sysprop: java.iccprofile.path.	2.3 Modifying build files or options	
FLINK-7274	Caused by: org.apache.flink.api.common.io.ParseException	1. Crash	FLINK-7050	Unresolved	"So, from my point of view this is expected behavior.

In my opinion, it isn't expected behaviour

The CsvInputFormat was never designed to adhere to the RFC specification. "	2.2 Incompatible issues	A workaround would be to read the quoted doubles as String and later convert them to double.	3.9 Modifying API calls	
FLINK-5946	"Kinesis Producer uses KPL that orphans threads that consume 100% CPU
"	6. Performance issue	none	none	"This issue is rooted in the Amazon Kinesis Producer Library (KPL) that the Flink Kinesis streaming connector depends upon.

The fix for this issue is included in release `0.12.4` of the AWS KPL, released 2 days ago (May 17, 2017). Anyone affected by this issue can use version `0.12.4` of the KPL. Marking this issue as closed."	"3.1 Problems in my libraries
1.2 Fixed by newer versions"	"The fix for this issue is included in release `0.12.4` of the AWS KPL, released 2 days ago (May 17, 2017). Anyone affected by this issue can use version `0.12.4` of the KPL."	3.3 Switching to newer versions	
FLINK-5898	java.lang.RuntimeException:	1. Crash	none	none	"The Flink Kinesis streaming-connector uses the Amazon Kinesis Producer Library (KPL) to send messages to Kinesis streams. 

The KPL tries to prevent multiple processes from extracting the binary at the same time by wrapping the operation in a mutex. Unfortunately, this does not prevent multiple Flink cores from trying to perform this operation at the same time. If two or more processes attempt to do this at the same time, then the native binary in /tmp will be corrupted."	3.1 Problems in my libraries	"The fix for this issue is included in release `0.12.4` of the AWS KPL, released 2 days ago (May 17, 2017). Anyone affected by this issue can use version `0.12.4` of the KPL. "	3.3 Switching to newer versions	
FLINK-5673	java.lang.NullPointerException	1. Crash	none	none	This is directly caused by the JobManager's local mode which is covered by FLINK-6487.	1.1 Fixed by related issues	This is directly caused by the JobManager's local mode which is covered by FLINK-6487.	1.1 Fixed problems	
FLINK-5633	I¡¯m running a job on my local cluster and the first time I submit the job everything works but whenever I cancel and re-submit the same job it fails 	3. Build and testing error	none	none	"When you see an exception in the style com.foo.X cannot be cast to com.foo.X, it means that multiple versions of the class com.foo.X have been loaded by different class loaders, and types of that class are attempted to be assigned to each other."	2.7 Problems in settings	I fixed the issue by not using the SpecificData.INSTANCE singleton (even though this is considered a common and expected practice). I feed every parser a new instance of SpecificData. This way the class cache is confined to a parser instance 	3.9 Modifying API calls	https://flink.apache.org/gettinghelp.html#i-see-a-classcastexception-x-cannot-be-cast-to-x
FLINK-12462	 I get a: java.lang.ClassNotFoundException	1. Crash	none	none	I think there is nothing we can do because Scala is breaking binary compatibility between minor versions.	4.2 Problems in languages	It works when changing the Flink version to 1.7.2. 	3.2 Switching to older versions	
FLINK-11641	flink seems to be stuck	"4. Hang

"	none	none	"adding memory at `flink-conf.yaml`
`taskmanager.heap.size: 4096m`
`jobmanager.heap.size: 2048m`"	2.7 Problems in settings	"I did solve it by adding memory 
`taskmanager.heap.size: 4096m`
`jobmanager.heap.size: 2048m`"	2.2 Modifying my settings	
FLINK-10960	"I started seeing the following error killing the job whenever a match was detected:

java.lang.RuntimeException"	1. Crash	none	none	"t's tricky to provide the example

but I haven't managed to recreate the error locally"	2.3 Flaky problems	I think we should add a disclaimer in the documentation which operations will result in state incompatibility.	2.1 Modifying documents	
FLEX-35363	"BUILD FAILED
C:\bin\apache-flex-sdk-4.16.0\installer.xml:351: The following error occurred while executing this line:
C:\bin\apache-flex-sdk-4.16.0\installer.xml:371: The following error occurred while executing this line:
C:\bin\apache-flex-sdk-4.16.0\installer.xml:891: The following error occurred while executing this line:
C:\bin\apache-flex-sdk-4.16.0\installer.xml:916: The following error occurred while executing this line:
C:\bin\apache-flex-sdk-4.16.0\installer.xml:923: The following error occurred while executing this line:
C:\bin\apache-flex-sdk-4.16.0\installer.xml:963: java.net.ConnectException: Connection timed out: connect"	3. Build and testing error	https://issues.apache.org/jira/secure/attachment/12893729/flex_install.log	none	"direct download was successful !

So it was likely my companies firewall."	4.5 Problems in the network	Can you click on that url and see if it's download: http://ftp.ps.pl/pub/apache/flex/4.16.1/binaries/apache-flex-sdk-4.16.1-bin.zip	2.3 Modifying build files or options	
FLEX-35309	Google Maps does not work	2. Unexpected behavior	none	none	"WebKit in AIR is so far out of date that I suspect that many major libraries are starting to have issues because they've dropped support for other browsers from that era. Apparently, it's too much work to constantly update WebKit with Adobe's internal changes."	3.1 Problems in my libraries	I did some research and found https://github.com/tuarua/WebViewANE which does just that - a fully integrated Chrome browser in an ANE.	3.1 Switching to other libraries	
FLEX-34754	The Spark VideoPlayer/VideoDisplay components do not display video.	2. Unexpected behavior	none	none	"Switching to Flex SDK 4.10 or older immediately resolves the problem. This bug can be replicated using Flex SDK 4.11, 4.12, 4.12.1, 4.13 and 4.14. "	2.4 Recurring problems	One workaround is to use old version of osmf.swf.	3.2 Switching to older versions	
FLEX-34733	Missing file in SDK (Prevents Compilation)	3. Build and testing error	none	none	After Installing Flex 4.14.0 w/Air 16 a file is missing	4.3 Problems in languages	Simple fix in the mean time is to simply copy that file from a Air 15 installation	2.3 Modifying build files or options	
FINERACT-235	"Expected result:
It should not collect any interest if the foreclosed date is same as disbursement date.
Actual result:
It is collecting one month interest."	2. Unexpected behavior	none	none	 if the foreclosed date is same as disbursement date.	2.5 Borderline cases	Work around is to  waive interest before closing it. 	3.9 Modifying API calls	
DRILL-6906	File permissions are not being honored	5. Security threat	none	none	" below variable was expected to be set in drill-env.sh for impersonation to work:

export MAPR_IMPERSONATION_ENABLED=${MAPR_IMPERSONATION_ENABLED:-""true""}"	2.7 Problems in settings	" below variable was expected to be set in drill-env.sh for impersonation to work:

export MAPR_IMPERSONATION_ENABLED=${MAPR_IMPERSONATION_ENABLED:-""true""}"	2.2 Modifying my settings	
DRILL-4177	"Node ran out of Heap memory, exiting.java.lang.OutOfMemoryError: GC overhead limit exceeded"	1. Crash	none	none	"Adding configuration properties ?useCursorFetch=true&defaultFetchSize=10000 [1] helps to solve OOM

Fetch size can be adjusted.

[1] https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-configuration-properties.html
[2] https://dev.mysql.com/downloads/connector/j/"	3.1 Problems in my libraries	"Adding configuration properties ?useCursorFetch=true&defaultFetchSize=10000 [1] helps to solve OOM

Here is some guidelines:
1. Before setting up storage plugin for MySql "	3.4 Modifying the settings of libraries	
DRILL-4171	"Node ran out of Heap memory, exiting.
java.lang.OutOfMemoryError: GC overhead limit exceeded"	1. Crash	DRILL-4177	RESOLVED	Problems seems the same as indicated in Jira DRILL-4177. Please take a look at comment in DRILL-4177 about adding configuration properties to enable batch read from MySql.	1.1 Fixed by related issues	Problems seems the same as indicated in Jira DRILL-4177. Please take a look at comment in DRILL-4177 about adding configuration properties to enable batch read from MySql.	3.4 Modifying the settings of libraries	
DIRSTUDIO-1173	The connection failed 	1. Crash	none	none	"if I use the ""Apache Directory LDAP Client API"" Provider instead of JNDI, it works! 

The difference is that JNDI is antiquated, dead in the water, and crappy."	4.4 Incorrect techniques.	"Switching from JNDI to the ""Apache Directory LDAP Client API"" solves this for us."	4.3 Switching to other techniques	
DIRSTUDIO-1143	"Stack trace when saving configuration

Caused by: java.lang.NullPointerException"	1. Crash	none	none	"The reason for this error is that in the config you attached the entries for the non-LDAP servers (ads-serverId=[changePassword|http|kerberos]Server,ou=servers,ads-directoryServiceId=default,ou=config) are missing

A second problem: You custom partition config LDAP entry looks weird, the ads-partitionid attribute value in the RDN differs from the value of the attribute."	2.7 Problems in settings	"I'm not sure how the Kerberos configuration was incomplete. (I inherited this LDAP server from someone else.) I put the LDIF you had in your comment into a file and loaded it with ldapadd. That helped clear up the Error 0 issues.

Secondly, I changed via Apache Directory Studio the ad-partitionid attribute for the dn: ads-partitionId=edrn,ou=partitions,ads-directoryServiceId=default,ou=config entry in Apache Directory Studio"	2.2 Modifying my settings	
DIRSTUDIO-1142	ApacheDS Server from previous version will not start.	1. Crash	none	none	 I know that's not handy but there is no smooth upgrade path between milestone versions.	2.5 Borderline cases	"The only way to upgrade and to keep your data is to run the old version again, do an LDIF export, upgrade to the new version, create a new server, and reimport the data."	4.3 Switching to other techniques	
DIRSTUDIO-1113	Unable to connect to LDAP server through SSL 	2. Unexpected behavior	none	none	due to US export restrictions (at least as far as I understand) we are not allowed to release sofware that generates a stronger key. So you need generate and import a stronger key yourself	2.2 Incompatible issues	a workaround would be to delete all values from jdk.certpath.disabledAlgorithms in java.security.	3.8 Deep copying	https://directory.apache.org/apacheds/basic-ug/3.3-enabling-ssl.html
DIRKRB-615	Can not connect to TCP simplekdc server	2. Unexpected behavior	none	none	DIRKRB-629	1.1 Fixed by related issues	"It's the issue in client side, so I suggest you can use the Kerby kinit.

Confirmed tool-dist/bin/kinit.sh works."	4.3 Switching to other techniques	https://directory.apache.org/kerby/user-guide/2.4-simplekdcserver.html
DERBY-7043	"DROP SCHEMA schemaname RESTRICT results in:

    ERROR XSAI2: The conglomerate (1,024) requested does not exist."	2. Unexpected behavior	none	none	 Do not know how or when the c400.dat file was deleted from seg0 directory. 	4.3 Problems in languages	"Workaround:

1) Re-downloaded Derby version 10.8.2.2.

2) Created a new database

3) Restored c400.dat from that new database to my other Derby database."	2.3 Modifying build files or options	
CXF-8103	OpenApiFeature - cannot use useContextBasedConfig	2. Unexpected behavior	https://github.com/swagger-api/swagger-core/pull/3283	CLOSED	"It looks like the regression caused by recent Swagger versions, we'll be looking into it, but the workaround for now would be to downgrade it to 2.0.6 

Before 2.0.7 it was the case but since than the Swagger Core changed implementation from Reflections to ClassGraph, and now always scan the complete class path if resource classes and resource packages are empty. The classes from the Application instance are added on top and there is no way to tailor this behavior."	"3.1 Problems in my libraries
2.4 Recurring problems"	"Add the any package to OpenApiFeature, for example
feature.setResourceClasses(Collections.singleton(""*""));
In this case, the classes will be properly picked up from Application instances."	2.2 Modifying my settings	
CXF-7722	CXF Swagger2Feature does not show methods if context root is not empty.	2. Unexpected behavior	none	none	"This issue appears to be working correctly in CXF 3.2.5-SNAPSHOT, but it exists in CXF 3.1.16-SNAPSHOT. Is it an option to upgrade? CXF 3.1.x uses an older version of Swagger UI which might be causing the problem. "	"3.1 Problems in my libraries
1.2 Fixed by newer versions"	"is not an issue any more when I set ""scan"" property to ""false""."	2.2 Modifying my settings	
CXF-7685	"WSDL2java -compile fails when run with JDK-9, JDK-10, JDK-11"	3. Build and testing error	none	none	"Why isn't system property, ""org.apache.cxf.common.util.Compiler-fork""  allowed to be passed as an input arg to WSDLToJava?  And this property is not documented and should be."	3.2 Problems in my clients	"Why isn't system property, ""org.apache.cxf.common.util.Compiler-fork""  allowed to be passed as an input arg to WSDLToJava?  And this property is not documented and should be."	3.9 Modifying API calls	
CXF-7308	XML Signature verification has failed	2. Unexpected behavior	none	none	"I managed to find a Weblogic expert, who told me to add this flag on server startup:

-DuseSunHttpHandler=true

That change fixed the problem."	3.1 Problems in my libraries	"I managed to find a Weblogic expert, who told me to add this flag on server startup:

-DuseSunHttpHandler=true

That change fixed the problem."	3.4 Modifying the settings of libraries	
CXF-7127	"I do not see the ""Origin"" header on the server when the request gets there."	2. Unexpected behavior	none	none	You need to force it to use the HttpClient:	3.2 Problems in my clients	"1. Add a dependency to the cxf-rt-transports-http-hc module:

<dependency>
  <groupId>org.apache.cxf</groupId>
  <artifactId>cxf-rt-transports-http-hc</artifactId>
  <version>${cxf.version}</version>
</dependency>
2. Set the ""use.async.http.conduit"" property:"	"3.1 Switching to other libraries
3.4 Modifying the settings of libraries"	http://cxf.apache.org/docs/client-http-transport-including-ssl-support.html
CXF-6931	"we get the following error:
java.lang.ClassCastException"	1. Crash	none	none	Root cause was xmlsec jars had javax classes. We used the xmlsec-2.0.0 jars and it worked.	"3.1 Problems in my libraries
1.2 Fixed by newer versions"	We used the xmlsec-2.0.0 jars and it worked.	3.3 Switching to newer versions	
CXF-6747	"Starting with version 3, roundtrip wsdl -> Java -> wsdl produces output wsdl different from wsdl on input"	2. Unexpected behavior	none	none	This is due to a bug in JAXB: https://java.net/jira/browse/JAXB-986	3.1 Problems in my libraries	"We have a workaround via a custom JAXB plugin ""cxf-xjc-bug986"""	3.1 Switching to other libraries	
CXF-5773	"Request Validation failed for Request with SimpleType parts

Response is
<soap:Envelope xmlns:soap=""http://schemas.xmlsoap.org/soap/envelope/"">"	2. Unexpected behavior	none	none	This is not something we can fix as JAXB does not provide methods to be able to resolve this directly.	3.1 Problems in my libraries	"if you generated the object model from the schema using a jaxb binding file that forces the generation of types for the simple types (instead of mapping to String), then this works as JAXB has the information it needs.

<jaxws:bindings xmlns:jaxws=""http://java.sun.com/xml/ns/jaxws"" xmlns:xs=""http://www.w3.org/2001/XMLSchema"" xmlns:jaxb=""http://java.sun.com/xml/ns/jaxb"" xmlns:wsdl=""http://schemas.xmlsoap.org/wsdl/"" wsdlLocation="""">
    <jaxws:bindings node=""wsdl:definitions/wsdl:types/xs:schema"">
<jaxb:globalBindings mapSimpleTypeDef=""true"">
</jaxb:globalBindings>
    </jaxws:bindings>
</jaxws:bindings>"	"4.3 Switching to other techniques
3.4 Modifying the settings of libraries"	https://cxf.apache.org/docs/jaxb.html
CTAKES-413	"org.apache.uima.tools.cvd.MainFrame.handleException(526): ????: An object of class org.apache.uima.resource.ResourceSpecifier was requested, but the XML input contained an object of class org.apache.uima.collection.impl.metadata.cpe.CpeDescriptionImpl."	1. Crash	none	none	"if you want to use a CPE descriptor like DrugNER_PlainText_CPE.xml, you would need to use runctakesCPE.bat rather than runctakesCVD.bat"	4.4 Incorrect techniques	"if you want to use a CPE descriptor like DrugNER_PlainText_CPE.xml, you would need to use runctakesCPE.bat rather than runctakesCVD.bat"	4.3 Switching to other techniques	
CTAKES-409	"While running a program to convert clinical note to snomed ct in multi threaded environment I got following exception.

org.apache.uima.analysis_engine.AnalysisEngineProcessException: Annotator processing failed."	2. Unexpected behavior	none	none	I have fixed this issue by modifying uimaj-core -> PrimitiveAnalysisEngine_impl.class -> callAnalysisComponentProcess() made thread safe. 	3.1 Problems in my libraries	"I have fixed this issue by modifying uimaj-core -> PrimitiveAnalysisEngine_impl.class -> callAnalysisComponentProcess() made thread safe.

Vighnesh posted workaround / fix in uima."	3.8 Deep copying	http://javadox.com/org.apache.uima/uimaj-core/2.6.0/org/apache/uima/analysis_engine/impl/PrimitiveAnalysisEngine_impl.html#callAnalysisComponentProcess(org.apache.uima.cas.CAS)
CTAKES-363	"This worked perfectly in v3.1.1 but now it is giving me an error :

'org.apache.uima.resource.ResourceInitializationException:Initialization
of CAS processor with name ""SimulatedProdAggregateTAE"" failed."	1. Crash	none	none	The 3.1.1 convenience binary incorrectly has a path with CH150124 in it.	3.2 Problems in my clients	"One workaround for 3.2.2 is as follows:
Copy the directory ctakes-smoking-status-res\src\main\resources\org from ctakes-smoking-status-res to under CTAKES_HOME\desc
so you have a directory CTAKES_HOME\desc\org
If you don't have a local copy of the source of ctakes-smoking-status-res, you can check out the needed directory from svn at
https://svn.apache.org/repos/asf/ctakes/tags/ctakes-3.2.2-rc2/ctakes-smoking-status-res/src/main/resources/org

Create CTAKES_HOME\desc\org\apache\ctakes\core
Copy the directory analysis_engine from within ctakes-core\desc to under CTAKES_HOME\desc\org\apache\ctakes\core
Again, if you don't have the source version of 3.2.2. you can check out the needed directory from svn from
https://svn.apache.org/repos/asf/ctakes/tags/ctakes-3.2.2-rc2/ctakes-core/desc/analysis_engine/"	2.4 Repairing as technical debts	
COMPRESS-471	Zipped files names having non UTF-8 encoding are being replaced with '?' while previewing file.	2. Unexpected behavior	none	none	"Unfortunately the ZIP format doesn't have a way of signalling the name encoding at all - which is not the complete truth, modern archiving libraries that use UTF-8 will indicate this by setting a flag and Commons Compress will use the flag to override the configured encoding, but not all archivers set this flag.

The main problem with your code is there is no way for an archive to say which encoding has been used when it has been created. getEncoding returns the encoding you specify as constructor argument - or in absence of such an argument it will always be UTF-8,"	2.1 Flawed repairs	"I would like library to set some flag when there are non-UTF8 characters in ZipFile means

or there could be some boolean value which tell whether non-utf8 characters are present in zip file or not.

I found below solution -"	3.6 Implementing wrappers	https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/archivers/zip/ZipArchiveEntry.html#getRawName--
COMPRESS-449	 is.skip() and IOUtils.skip() are throwing an IOException somewhere somehow. 	1. Crash	none	none	"I think this might be a lower-level bug with my java-vm.

Unfortunately there is no way to tell whether a stream supports seek, and for some reason the wrapped FileInputStream feels like throwing an exception rather than returning 0 (which IOUtils.skip would handle)."	4.2 Problems in languages	"Rather we recommend you wrap System.in in something like https://github.com/apache/commons-compress/blob/master/src/main/java/org/apache/commons/compress/utils/SkipShieldingInputStream.java 

provide a custom InputStream for special skip problems"	3.6 Implementing wrappers	
CELIX-346	"celix-bootstrap problems

IndentationError: unindent does not match any outer indentation level"	1. Crash	none	none	Removing celix-boottstrap from develop branch. I expect the INAETICS CPF will replace this. 	1.1 Fixed by related issues	Removing celix-boottstrap from develop branch. I expect the INAETICS CPF will replace this. .	4.3 Switching to other techniques	
CB-13038	causing a delay in content rendering in the correct mode. 	6. Performance issue	none	none	Don't use Javascript for layout since you can't control the speed of the JS execution	4.4 Incorrect techniques	use CSS for style and layout. CSS media queries will always be faster than listening to events in Javascript. 	"4.3 Switching to other techniques
"	
CB-12688	XHR to local network with CSP ms-appx works on phone but not on PC (Windows 10 UWP)	2. Unexpected behavior	none	none	"This way the app would never pass the app store check, but at least I can make sure XHR has local access and it works immediately. So it cannot be some ""side effect"""	2.1 Flawed repairs	"<platform name=""windows"">
<config-file target=""package.appxmanifest"" parent=""/Package/Capabilities"">
<Capability Name=""privateNetworkClientServer"" />
</config-file>
</platform>

It works!!!"	3.4 Modifying the settings of libraries	https://cordova.apache.org/docs/en/5.1.1/guide/platforms/win8/win10-support.html
CB-12413	Unable to properly sign app with a trusted certificate	2. Unexpected behavior	none	none	This one isn't signed with a trusted certificate	2.7 Problems in settings	"I resolved it by removing the store association, doing a clean build and then reassociating the app with the store."	2.3 Modifying build files or options	
CB-12113	"Device plugin and crosswalk plugin, no deviceready event"	2. Unexpected behavior	none	none	I've found the problem. The 2.1.0 version of cordova-plugin-crosswalk-webview is broken	3.1 Problems in my libraries	As a quick workaround you can revert to cordova-android 5.2.2.	3.2 Switching to older versions	
CB-12078	"Project setup behind proxy no longer possible with android@6.0.0
"	3. Build and testing error	none	none	"I hate recommending this, because hooks make things more complex, and far harder to support"	2.1 Flawed repairs	What you may want to do is add a hook that adds the gradle.properties file instead with the proper information.	4.3 Switching to other techniques	https://cordova.apache.org/docs/en/latest/guide/appdev/hooks/
CB-12037	"wkwebview ignoring KeyboardDisplayRequiresUserAction = false

That allows programmatic setting of focus on inputs to show the keyboard."	5. Security threat	none	none	"This is because there is no equivalent, supported, setting for WKWebView. Apple hasn't provided one."	3.1 Problems in my libraries	"Apple hasn't provided one.

There is an alternative:
https://github.com/Telerik-Verified-Plugins/WKWebView/issues/184"	3.1 Switching to other libraries	
CB-12002	Support LSApplicationQueriesSchemes	2. Unexpected behavior	none	none	"LSApplicationQueriesSchemes is only needed for canOpenURL, but we use openURL "	2.1 Flawed repairs		2.4 Repairing as technical debts	
CB-11974	wkwebview LocalStorage Persistence For Existing Users	2. Unexpected behavior	none	none	I got tired of waiting for this issue to be addressed here so I built a companion plugin for handling LocalStorage data migration from UIWebView to WKWebView: https://github.com/maklesoft/cordova-plugin-migrate-localstorage	2.6 Outside contributors	I got tired of waiting for this issue to be addressed here so I built a companion plugin for handling LocalStorage data migration from UIWebView to WKWebView:	2.4 Repairing as technical debts	
CB-11449	We tap on the form field and it does not allow any inputs.	2. Unexpected behavior	CB-8049	CLOSED	"we downgrade to cordova 3.7.0 and use ""cordova-plugin-wkwebview"", it works as intended.

this is likely an Apple rendering issue in UIWebView"	"4.1 Problems in operating systems
2.4 Recurring problems"	"We've downgrade to cordova 3.7.0 and use ""cordova-plugin-wkwebview"", it works as intended."	"3.2 Switching to older versions
3.1 Switching to other libraries"	
CB-10950	In android application when the user tries for capturing images and select image from gallery app stop/restart.	1. Crash	none	none	This is a known problem that sometimes happens in low memory conditions. The Android OS will sometimes kill applications in the background to free up memory.	4.1 Problems in operating systems	"Unfortunately, there is no way to stop the OS from killing your app in the background on devices with low memory. Even in memory efficient apps it can still happen sometimes.

The best you can do from a Cordova app is to handle the lifecycle and restore your app's state when it occurs. The code in the guide I linked is meant to provide an example of how to do that.  "	3.9 Modifying API calls	https://cordova.apache.org/docs/en/9.x/guide/platforms/android/index.html#lifecycle-guide
CB-10939	 I can't not use things like navigator.camera.getPicture or navigator.notification.alert.	2. Unexpected behavior	none	none	"Short version:
add <allow-navigation href=""*"" /> to config.xml solved the problem."	2.7 Problems in settings	"Short version:
add <allow-navigation href=""*"" /> to config.xml solved the problem."	2.2 Modifying my settings	
CAY-2147	Error Generating DB Schema for postgresql database	7. Warning-style error 	none	none	"The problem is in schema camelCase naming as driver converts unquoted identifiers to lowercase.
However ""Quote SQL Identifiers"" checkbox in DataMap settings fixes this. When it's ""on"" this query is executed without error."	3.1 Problems in my libraries	"However ""Quote SQL Identifiers"" checkbox in DataMap settings fixes this. When it's ""on"" this query is executed without error."	3.4 Modifying the settings of libraries	
CASSANDRA-13212	" when I try to run an embedded Cassandra unit test, it always fails with the following error:

Exception (java.lang.NoClassDefFoundError) "	1. Crash	none	none	Configure SpringBoot and force it to use Log4J2 instead of attempting to autodetect the logging framework to use.	3.1 Problems in my libraries	Configure SpringBoot and force it to use Log4J2 instead of attempting to autodetect the logging framework to use.	3.4 Modifying the settings of libraries	
CAMEL-9463	Camel-guice OSGi dependency resolution fails	1. Crash	none	none	"Camel-guice feature file says it requires com.google.inject/guice/4.0. That bundle do not export com.google.inject.internal.* classes that are required by camel-guice bundle. 

It seems the bundle produced by Google with Maven-bundle-plugin exclude *.internal from being exported."	3.1 Problems in my libraries	"It seems that there is no equivalent ServiceMix version for guice-4

Using Guice 3.x on OSGi that works."	3.2 Switching to older versions	
CAMEL-9107	 Caused by: [org.apache.camel.RuntimeCamelException - Failed to extract body due to: Unable to load BODYSTRUCTURE	1. Crash	none	none	"so isn't all this finally a bug in javamail ?

Yes its in java mail that has a bug"	3.1 Problems in my libraries	 catch to ignore when attempting to check the email for attachments. But this would cause that you cannot get the attachments from those invalid emails.	3.7 Bypassing APIs with bugs	
CAMEL-13089	the service is not starting anymore.	1. Crash	none	none	Already reported this in https://issues.jboss.org/browse/THORN-2314 but they claim to not support Camel in Thorntail.	3.2 Problems in my clients	We are currently using org.apache.camel:camel-rabbitmq and org.apache.camel:camel-cdi instead.	3.1 Switching to other libraries	
CAMEL-12668	The problem here is when I stop the context the remaining aggregated exchanges are not being flushed to the processor. 	2. Unexpected behavior	none	none	"you can try to use shutdownRoute(Defer) option as workaround, but it doesn't solve a core problem. If it helps in your case I will close this issue and open a new one with the root case description."	2.1 Flawed repairs	"you can try to use shutdownRoute(Defer) option as workaround,"	3.9 Modifying API calls	
CAMEL-12367	"thread created by ServiceHelper.startService is never going to be killed, and since it's never going to be reused because we'll never have twice the same endpoint URI, eventually the application dies because of the rogue threads."	1. Crash	none	none	"I think we should create a new ticket about a new S3PollingConsumer (and potentially a few other in the camel-aws) as the AWS component is much in use, and could benefit from this. Implementing S3PollingConsumer is a bit ""more hard"" to do."	2.1 Flawed repairs	"You can set the cacheSize option to 0 or -1 to disable any cached consumers

The solution is to reduce the cache size as previously documented here"	3.4 Modifying the settings of libraries	
BROOKLYN-588	SoftwareProcess download with curl can fail on CentOS 7.0 (TLS negotiation)	3. Build and testing error	none	none	I think we're hitting https://bugzilla.redhat.com/show_bug.cgi?format=multiple&id=1170339	4.1 Problems in operating systems	the recommended workaround is to use a more recent version of CentOS. 	"4.1 Modifying operating systems
2.1 Modifying documents"	
BEAM-6460	" When the pipeline is restarted due to a failure, a new classloader is created which can result in too many classes being loaded."	2. Unexpected behavior	none	none	"Jackson Cache may hold on to Classloader after pipeline restart

Not a Beam issue. Flink leaks classloaders when libraries are loaded through the Flink root classloader which have static caches. "	3.1 Problems in my libraries	The subtlety of this issue warrants that we clear the Jackson cache to avoid other users running into this. 	"3.7 Bypassing APIs with bugs
"	
BEAM-6289	The timeout then results in a shutdown of the local executor.	1. Crash	none	none	The Cassandra source takes too long to generate the splits which is done at the JobManager when the Beam job is sent to the cluster. This lets the job submission time out because it does not receive the acknowledgement from the JobManager until the splits are done. The timeout then results in a shutdown of the local executor. 	3.1 Problems in my libraries	"Increase the Flink setting for the Akka timeout, i.e. akka.ask.timeout: 1 minute. You will have to set the environment variable FLINK_CONF_DIR with the directory containing the ""flink-conf.yaml"". "	3.4 Modifying the settings of libraries	
BEAM-3525	tests will be run locally incorrectly	2. Unexpected behavior	none	none	TestPipeline serializes PipelineOptions prematurely	3.2 Problems in my clients	Solution: Remove JsonIgnore annotation from runner configuration options.	2.2 Modifying my settings	
BEAM-2268	NullPointerException in com.datatorrent.netlet.util.Slice via ApexStateInternals	1. Crash	none	none	this is a certain NPE in netlet 1.2.1 but our dependencies are on 1.3.0.	2.7 Problems in settings	Please remove the netlet dependency from your local maven repo	2.2 Modifying my settings	
BEAM-1048	Spark Runner streaming batch duration does not include duration of reading from source	2. Unexpected behavior	none	none	"This is not a bullet proof solution, as technically there is no guarantee that at a given point in time, the read duration and process delay metrics actually refer to the same batch id"	2.1 Flawed repairs	"This is not a bullet proof solution, as technically there is no guarantee that at a given point in time, the read duration and process delay metrics actually refer to the same batch id"	2.4 Repairing as technical debts	
ATLAS-1057	Build break	3. Build and testing error	none	none	"I get an error building titan which looks like a dependancy issue. 

Error resolving project artifact: Could not transfer artifact org.restlet.jee:org.restlet:pom:2.3.0 from/to apache.snapshots.repo

 this is a download that is taking too long.
This indicates that repeating the build worked. It fails for me still - but I am using -clean which would wipe any downloaded files."	4.3 Problems in languages	I successfully rebuilt without -clean	"2.3 Modifying build files or options
"	
ASTERIXDB-1767	Excessive log output for integration tests	7. Warning-style error	none	none	The log level was lowered but it might be better to have less stuff print at INFO overall.	2.7 Problems in settings	The log level was lowered	2.2 Modifying my settings	https://asterixdb.apache.org/docs/0.9.4.1/ncservice.html
ASTERIXDB-1675	all functions on Documentation will throw this error. 	1. Crash	none	none	I guess that maybe there is something changed in the Metadata format that I need to reingest the data?	3.3 Incorrect inputs	"build again

CREATE DATAVERSE Default;"	"2.3 Modifying build files or options
"	
ASTERIXDB-1518	Integration test keeps restarting instance in asterix-yarn tests	3. Build and testing error	none	none	"I believe this is fixed, was related to storage artifacts not being cleaned in the job."	2.7 Problems in settings	"I believe this is fixed, was related to storage artifacts not being cleaned in the job."	"2.3 Modifying build files or options
"	
ARTEMIS-2184	AIO does not start on tmpfs: RuntimeException	1. Crash	none	none	"O_DIRECT flag is unsupported on tmpfs.

AIO requires O_DIRECT. If you want to put the journal on tmpfs then I recommend trying NIO."	4.4 Incorrect techniques	AIO requires O_DIRECT. If you want to put the journal on tmpfs then I recommend trying NIO.	4.3 Switching to other techniques	https://activemq.apache.org/components/artemis/documentation/1.0.0/running-server.html
ARTEMIS-2132	Multicast consumer via HTTP transport: AMQ212037 Connection failure has been detected	2. Unexpected behavior	none	none	"Some quick testing indicates this failure was introduced in 2.6.2 which
means if you use 2.6.1 it should work."	2.4 Recurring problems	"Downgrade to Artemis 2.6.1
Configure confirmationWindowSize to something greater than 0 in the ConnectionFactory URL"	"3.2 Switching to older versions
2.2 Modifying my settings"	https://activemq.apache.org/components/artemis/documentation/2.8.0/clusters.html
ARROW-5539	"Tests run: 6, Failures: 0, Errors: 2, Skipped: 0,"	3. Build and testing error	none	none	Not sure what the underlying issue is	2.3 Flaky problems	"Generally I do ""mvn clean install"" when I get this.

You're right, that worked."	"2.3 Modifying build files or options
"	
ARROW-192	some classes under package arrow Vector has compiling error	3. Build and testing error	none	none	some classes under package arrow Vector has compiling error because some dependency class cannot be resolved	2.7 Problems in settings	"As a workaround, ou can run manually mvn generate-sources to create those, and refresh your project in the IDE."	"2.3 Modifying build files or options
"	
AMQ-6976	GC goes crazy under load. Heap is filled up with TransportConnectionState objects	6. Performance issue	none	none	"We figured out that we were using several connection factories for different queues/topics together with the ""is-same-rm-override"" flag NOT set."	3.1 Problems in my libraries	"We figured out that we were using several connection factories for different queues/topics together with the ""is-same-rm-override"" flag NOT set.
If we enable this flag by setting it to false, everything works fine."	3.4 Modifying the settings of libraries	
AMQ-6231	AbortSlowConsumer Strategy Inaccurate	2. Unexpected behavior	none	none	For this case it would be better to use the AbortSlowAckConsumerStrategy instead.	4.4 Incorrect techniques	 it would be better to use the AbortSlowAckConsumerStrategy instead.	4.3 Switching to other techniques	
AMQ-6162	"DLQ'd message does not get DLQ'd again until broker restart

message seems lost"	2. Unexpected behavior	none	none	"If you restart the broker between step 5 and step 6, the message does get DLQ'd to DLQ1 again.

The message will be filtered as a duplicate when the rollback attempts to place it back onto the DLQ1 queue in the no restart case."	2.5 Borderline cases	" the duplicates tracking should probably roll back the audit to allow the message to cycle again. 

You can work around this by disabling the Audit for your dead letter strategies which will prevent the tiered DLQ's from filtering out the message on repeated cycles through them.

I have disabled audit for the DLQs and the messages cycle through as expected now."	2.2 Modifying my settings	https://activemq.apache.org/message-redelivery-and-dlq-handling
AMQ-5702	REST production of message bodies can result in IllegalStateException	1. Crash	none	none	"On your GitHub code you set

If strMethod = ""POST"" Then
objWinHttp.SetRequestHeader ""Content-type"", _
""application/x-www-form-urlencoded""
End If

on the Wiki this issue is addressed by setting the header as:

""Content-Type: text/plain"""	3.3 Incorrect inputs	"On your GitHub code you set

If strMethod = ""POST"" Then
objWinHttp.SetRequestHeader ""Content-type"", _
""application/x-www-form-urlencoded""
End If

on the Wiki this issue is addressed by setting the header as:

""Content-Type: text/plain"""	3.11 Modifying input format	https://cwiki.apache.org/confluence/display/ACTIVEMQ/REST
AMBARI-15971	"Exception in ambari-metrics-collector.log,can't find java class"	1. Crash	none	none	"It seems lack of phoenix-HBase-server jar.And When put this jar in /usr/lib/ams-hbase/lib diractory, exception gone."	2.7 Problems in settings	"It seems lack of phoenix-HBase-server jar.And When put this jar in /usr/lib/ams-hbase/lib diractory, exception gone."	"2.3 Modifying build files or options
"	
AIRFLOW-884	SlackAPIPostOperator works inconsistently across two DAGs / Python scripts.	2. Unexpected behavior	none	none		2.3 Flaky problems		1.2 Unfixed problems	
AIRFLOW-522	python setup.py develop is not working on master	1. Crash	none	none	"Finished processing dependencies for airflow==1.7.2.dev0

but the CLI requires 1.7.1.3

pkg_resources.DistributionNotFound: The 'airflow==1.7.1.3' distribution was not found and is required by the applicati......"	2.7 Problems in settings		"2.3 Modifying build files or options
"	
AIRFLOW-436	"Found that when .format is used in a templatable field, it produces weird results. "	2. Unexpected behavior	none	none	By definition the default Jinja templating and string.format() are incompatible;	3.2 Problems in my clients	I suggest to use `string.Template` class instead of `string.format()` method to not conflict with the Jinja renderer	3.9 Modifying API calls	
AIRFLOW-43	I attempted to start this using `airflow trigger_dag NAME` but the tasks never run. 	1. Crash	none	none	"UTC ¨C yes in DB, no on the OS. Didn't realize this was a requirement. 

We know this can occur when the entire system is not running in UTC. Running in UTC everywhere is a current requirement of running Airflow."	4.1 Problems in operating systems	"UTC ¨C yes in DB, no on the OS. Didn't realize this was a requirement. 

We know this can occur when the entire system is not running in UTC. Running in UTC everywhere is a current requirement of running Airflow."	4.1 Modifying operating systems	"https://airflow.apache.org/docs/stable/changelog.html

https://cwiki.apache.org/confluence/display/AIRFLOW/Common+Pitfalls"
AIRFLOW-217	 I have configured all the settings but still it is not running the job. 	1. Crash	none	none	We were running different versions of Airflow on master and worker.	2.7 Problems in settings	Now we are using Ubuntu instances with same versions of Airflow on master and workers.	2.3 Modifying build files or options	
